export const cid = "fairness_metrics_toolbox_for_classification"
export const ib_cid = "fairness_tree"
export const gid = "aiverify.stock.fairness_metrics_toolbox_for_classification"
import { InterpretationChart } from './interpretationtype1.mdx';
import { InterpretationChart2 } from './interpretationtype2.mdx';


export const GetInterpretation = ({ data, container, results }) => {
  let metrics_map = {
    'False Negative Rate Parity': "False Negative Rate",
    "False Positive Rate Parity": "False Positive Rate",
    "False Discovery Rate Parity": "False Discovery Rate",
    "False Omission Rate Parity": "False Omission Rate",
    "True Positive Rate Parity / Equal Opportunity": "True Positive Rate",
    "True Negative Rate Parity": "True Negative Rate",
    "Positive Predictive Value Parity": "Positive Predictive Value Parity",
    "Negative Predictive Value Parity": "Negative Predictive Value Parity",
    "Equal Parity": "Equal Selection Parity",
    "Disparate Impact": "Disparate Impact",
  };
  let content = [];
  let metrics = data.metrics;
  
  for (var i = 0; i < metrics.length; i++) {
    // Special graph as they have more than 2 subgroups
    if (metrics[i] == "Equal Parity" || metrics[i] == "Disparate Impact"){
      content.push(<InterpretationChart2
        metric={metrics_map[metrics[0]]}
        container={container}
        data={results}
      />)
    }
    else{
      content.push(<InterpretationChart
        metric={metrics_map[metrics[i]]}
        container={container}
        data={results}
      />)
      }
    }

  return (
    <div>{content}</div>
  )
}

<>
  <div>
    <b>What it means:</b>
    <br/>
    The test results enable the business to understand if the model is able to predict the outcomes fairly among the target groups. 
    <GetInterpretation
      data={props.getIBData(ib_cid, gid)}
      container={props.container}
      results={props.getResults(cid, gid)}
    />
  </div>

<div>
    <h4><b>Recommendations:</b> </h4>

    <p>The test results enable the business to understand if the model is able to predict the outcomes fairly among the target groups.</p>
    <p>If the parity is unacceptable, consider doing the following:</p>
    <ol>
        <li>Review your dataset to identify any inherent bias in the dataset.</li>
        <li>Review your model parameters and algorithms.</li>
        <li>Apply post-processing mitigation algorithms</li>
    </ol>

    The potential use cases (non-exhaustive) are as follows:
<p><u>Detecting presence of unfair treatment of model to downstream competitors </u> </p>
<p>If the selected feature is “competitor”, for example, the business should ensure that the AI model's predictions do not result in a higher number of false positives for downstream competitors compared to its own subsidiary. By doing so, the business may be leveraging its market power in the upstream market to accord favourable treatment to its subsidiary. If the business is dominant, it should review the AI algorithm to avoid potentially infringing Section 47 of the Competition Act, which prohibits the abuse of a dominant market position that is not based on fair competition and perpetuates the dominant position of the business.</p>

<p><u>Detect presence of unfair treatment of model to specific demographics, which may result in false or misleading claims being made if not disclosed adequately.</u></p>
<p>If the selected feature is gender, for example, the business should ensure that the AI model's predictions do not result in a higher number of false positives for one gender over the other. Any limitation in the prediction of the model should be clearly and accurately informed to users, so that they would not be misled by the results. If the business is making false or misleading claims or failing to disclose any limitations, the business could be found to have engaged in an unfair practice under the Consumer Protection (Fair Trading) Act.</p>
</div>
</>
