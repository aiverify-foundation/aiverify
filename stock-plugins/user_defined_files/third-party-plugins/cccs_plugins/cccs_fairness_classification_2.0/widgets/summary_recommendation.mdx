export const cid = "fairness_metrics_toolbox_for_classification"
export const ib_cid = "fairness_tree"
export const gid = "aiverify.stock.fairness_metrics_toolbox_for_classification"
import { InterpretationChart } from './interpretationtype1.mdx';
import { InterpretationChart2 } from './interpretationtype2.mdx';


export const GetInterpretation = ({ data, container, results }) => {
  let metrics_map = {
    'False Negative Rate Parity': "False Negative Rate",
    "False Positive Rate Parity": "False Positive Rate",
    "False Discovery Rate Parity": "False Discovery Rate",
    "False Omission Rate Parity": "False Omission Rate",
    "True Positive Rate Parity / Equal Opportunity": "True Positive Rate",
    "True Negative Rate Parity": "True Negative Rate",
    "Positive Predictive Value Parity": "Positive Predictive Value Parity",
    "Negative Predictive Value Parity": "Negative Predictive Value Parity",
    "Equal Parity": "Equal Selection Parity",
    "Disparate Impact": "Disparate Impact",
  };
  let content = [];
  let metrics = data.metrics;
  
  for (var i = 0; i < metrics.length; i++) {
    // Special graph as they have more than 2 subgroups
    if (metrics[i] == "Equal Parity" || metrics[i] == "Disparate Impact"){
      content.push(<InterpretationChart2
        metric={metrics_map[metrics[0]]}
        container={container}
        data={results}
      />)
    }
    else{
      content.push(<InterpretationChart
        metric={metrics_map[metrics[i]]}
        container={container}
        data={results}
      />)
      }
    }

  return (
    <div>{content}</div>
  )
}

<>
  <div>
    <b>What it means:</b>
    <br/>
    The test results enable the Company to help its stakeholder understand if the model is able to predict the outcomes fairly among the demographic groups.
    <GetInterpretation
      data={props.getIBData(ib_cid, gid)}
      container={props.container}
      results={props.getResults(cid, gid)}
    />
  </div>

<div>
    <h4><b>Recommendations:</b> </h4>

    <p>The test results enable the company to understand if the model is able to predict the outcomes fairly among the target groups. If the parity is unacceptable, consider doing the following:</p>
    <ol>
        <li>Review your dataset to identify any inherent bias in the dataset.</li>
        <li>Review your model parameters and algorithms.</li>
        <li>Apply post-processing mitigation algorithms</li>
    </ol>

    The potential use cases (non-exhaustive) are as follows:
<p><u>Detecting presence of unfair treatment of model to downstream competitors </u> </p>
<p>If the selected feature is “competitor”, for example, the AI company should ensure that the AI model's predictions do not result in a higher number of false positives for downstream competitors compared to its own subsidiary. By doing so, the company may be leveraging its market power in the upstream market to accord favourable treatment to its subsidiary. If the company considers itself to be dominant, it should review the AI algorithm to avoid potentially infringing Section 47 of the Competition Act, which prohibits the abuse of a dominant market position that is not based on fair competition and perpetuates the dominant position of the business.</p>

<p><u>Detect presence of unfair treatment of model to specific demographics, which may result in false or misleading claims being made if not disclosed adequately</u></p>
<p>If the selected feature is "gender", for example, the AI company should ensure that the AI model's predictions do not result in a higher number of false positives for one gender over the other. Any limitation in the prediction of the model should be clearly and accurately informed to users, so that they would not be misled by the results. Any false or misleading claims made in this regard to consumers could be an unfair practice as defined under the Consumer Protection (Fair Trading) Act.</p>
</div>
</>
