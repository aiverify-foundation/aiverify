{
  "exportDate": "2025-08-14T09:33:52.810Z",
  "checklists": [
    {
      "id": "",
      "cid": "transparency_process_checklist",
      "name": "Transparency Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Transparency",
      "description": "Transparency provides visibility to the intended use and impact of the AI system. It complements existing privacy and data governance measures. Integrating transparency into the AI lifecycle helps ameliorate the problems caused by opaqueness. The testable criteria focuses on ensuring communication mechanisms are in place to enable those affected by AI systems to understand how their data is collected and used, as well as the intended use and limitations of the AI system. This should be done in a manner appropriate to the use case at hand and accessible to the audience.  ",
      "sections": [
        {
          "section": "Transparency",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Provide the necessary information to end users about the use of their personal data to ensure it is processed in a fair and transparent manner",
              "processes": [
                {
                  "pid": "1.1.1",
                  "process": "Align with (1) the PDPC’s Advisory Guidelines on Key Concepts in the PDPA; (2) Guide to Accountability; and (3) Guide to Data Protection Impact Assessments",
                  "metric": "Internal documentation (e.g., policy document)",
                  "processChecks": "Documentary evidence of internal policy requiring alignment with existing data protection laws and regulations, which include:  <br/>(in Singapore)<br/>- PDPC’s Advisory Guidelines on Key Concepts in the PDPA; <br/>- Guide to Accountability; and<br/>- Guide to Data Protection Impact Assessments.<br/>(outside Singapore)<br/>- Applicable data protection laws/regulations",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "1.1.2",
                  "process": "Publish a privacy policy on your organization’s website to share information about the use of personal data in the AI system (e.g., data practices, and decision-making processes). The general disclosure notice could include:<br/>– Disclosure of third-party engagement<br/>– Definition of data ownership and portability<br/>– Depiction of the data flow and identify any leakages<br/>– Identification of standards the company is compliant with as assurance to customers",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of a privacy policy on your organization’s website to share information about the use of personal data in the AI system (e.g., data practices and decision-making processes). <br/><br/>The general disclosure notice could include:<br/>– Disclosure of third-party engagement; <br/>– Definition of data ownership and portability;<br/>– Depiction of the data flow and identify any leakages; and<br/>– Identification of standards the company is compliant with as assurance to customers",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Where possible (e.g., not compromising IP, safety, or system integrity), identify appropriate junctures in the AI lifecycle to inform end users, subjects and other relevant parties about necessary information regarding the AI system (e.g., purpose, criteria, limitations and risks of the decision(s) generated by the AI system) in an accessible manner",
              "processes": [
                {
                  "pid": "1.2.1",
                  "process": "Design an in-house policy on communication to consumers that articulates the principles for transparency, e.g., define the purpose and context of communication to determine how and what to communicate",
                  "metric": "Internal documentation (e.g., policy document)",
                  "processChecks": "Documentary evidence of an in-house policy on communication to consumers that articulates the principles for transparency, e.g., define the purpose and context of communication to determine how and what to communicate",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "1.2.2",
                  "process": "Inform relevant stakeholders that AI is used in your products and/or services",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communication to relevant stakeholders that AI is used in the organisation's products and/or services",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "1.2.3",
                  "process": "For decisions made by the AI system, where possible, communicate to end users the factors leading to the decision e.g., \"You are being shown this product because you bought this item.\"",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communicating to end users the factors that lead to decisions made by AI systems",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "1.2.4",
                  "process": "Consult end users at the earliest stages of AI system development to communicate how the technology is used and how it will be deployed",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communication with end users at early stages of AI system development concerning how the technology is used and how it will be deployed",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "1.2.5",
                  "process": "Surface relevant information (e.g., regarding accuracy, intended use cases, and limitations of the AI system, including the risk assessment), to stakeholders such as end users and relevant authorities in alignment with regulations",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communication with stakeholders concerning the AI system, which includes (where applicable): <br/><br/>- accuracy;<br/>- confidence scores;<br/>- intended use cases;<br/>- limitations; and<br/>- risk assessment",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Provide information to guide end users on the proper use of the AI system in an accessible manner",
              "processes": [
                {
                  "pid": "1.3.1",
                  "process": "Provide information such as the purpose, intended use and intended response of the AI system to end users",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communication with end users concerning the intended use and intended response of the AI system (e.g., Model Card and Data Card)",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Provide end users and other external parties with the means to report adverse impacts of AI system to the organisation",
              "processes": [
                {
                  "pid": "1.4.1",
                  "process": "Processes in place for reporting of adverse impacts of AI system",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of processes by which end users and other external parties are able to report adverse impacts of AI sytem",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Communicate to end users whenever an incident has occurred",
              "processes": [
                {
                  "pid": "1.5.1",
                  "process": "Processes in place for communication of incidents to end users",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of communication plan",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": "Summary Justification"
    },
    {
      "id": "",
      "cid": "explainability_process_checklist",
      "name": "Explainability Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Explainability",
      "description": "Explainability is about ensuring AI-driven decisions can be explained and understood by those directly using the system to enable or carry out a decision to the extent possible. The degree to which explainability is needed also depends on the aims of the explanation, including the context, the needs of stakeholders, types of understanding sought, mode of explanation, as well as the severity of the consequences of erroneous or inaccurate output on human beings. Explainability is an important component of a transparent AI system. The testable criteria in this section focus on system-enabled explainability. However, it may not be possible to provide an explanation for how a black box model generated a particular output or decision (and what combination of input factors contributed to that). In these circumstances, other explainability measures may be required (e.g., accountability and transparent communication). As state-of-the-art approaches to explainability become available, users should refine the process, metrics, and/or thresholds accordingly. ",
      "sections": [
        {
          "section": "Explainability",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Demonstrate a preference for developing AI models that can explain their decisions or that are interpretable by default",
              "processes": [
                {
                  "pid": "2.1.1",
                  "process": "If choosing a less explainable modelling approach, document the rationale, risk assessments, and trade-offs of the AI model",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of considerations for the choice of AI model<br/><br/>Considerations include:<br/>- rationale; <br/>- risk assessment; and<br/>- trade-offs",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": "Summary Justification"
    },
    {
      "id": "",
      "cid": "reproducibility_process_checklist",
      "name": "Reproducibility Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Reproducibility",
      "description": "Reproducibility is a crucial requirement for achieving system resilience. With software systems, the ability to reproduce an outcome or error is key to identifying and isolating the root cause. The testable criteria in this section focus on logging capabilities to monitor the AI system, tracking the journey of a data input through the AI lifecycle, and reviewing the input and output of the AI system.",
      "sections": [
        {
          "section": "Traceability",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Put in place methods to record the provenance of the AI model, including the various versions, configurations, data transformations, and underlying source code",
              "processes": [
                {
                  "pid": "3.1.1",
                  "process": "Implement version control of source code and frameworks used to develop the model. For each version of the model, track the code version, as well as the parameters, hyperparameters, and source data used",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of version control of source code and frameworks used to develop the model, including considerations of how much version history is required<br/><br/>Each version of the model should track the following:<br/>- code version;<br/>- parameters; <br/>- hyperparameters; and <br/>- source data",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place measures to ensure data quality over time",
              "processes": [
                {
                  "pid": "3.2.1",
                  "process": "Verify the quality of data used in the AI system. This may include the following: <br/>- accuracy in terms of how well the values in the dataset match the true characteristics of the entity described by the dataset<br/>- completeness in terms of attributes and items e.g., checking for missing values, duplicate records<br/>- veracity in terms of how credible the data is, including whether the data originated from a reliable source<br/>- How recently the dataset was compiled or updated<br/>- Relevance for the intended purpose<br/>- Integrity in terms of how well extraction and transformation have been performed if multiple datasets are joined;<br/>- Usability in terms of how the data are tracked and stored in a consistent, human-readable format<br/>- Providing distribution analysis e.g., feature distributions of input data",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that proves due diligence has been done to ensure the quality of data. This can include the use of relevant processes or software that:<br/>- Conducts validation schema checks<br/>- Identifies possible errors and inconsistencies at the exploratory data analysis stage, before training the dataset<br/>- Assigns roles to the entire data pipeline to trace who manipulated data and by which rule<br/>- Allows for review before a change is made<br/>- Unit tests to validate that each data operation is performed correctly prior to deployment<br/>- Allow for periodic reviewing and update of datasets <br/>- Allow for continuous assessment of the quality of the input data to the AI system, including drift parameters and thresholds, where applicable",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place measures to understand the lineage of data, including knowing where the data originally came from, how it was collected, curated, and moved within the organisation over time",
              "processes": [
                {
                  "pid": "3.3.1",
                  "process": "Maintain a data provenance record to ascertain the quality of the data based on its origin and subsequent transformation. This could include the following: <br/>- Take steps to understand the meaning of and how data was collected<br/>- Document data usage and related concerns. <br/>- Ensure any data labeling is done by a representative group of labelers <br/>- Document the procedure for assessing labels for bias<br/>- Trace potential sources of errors<br/>-Update data<br/>- Attribute data to their sources",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of a data provenance record that includes the following info, where applicable:<br/>- clear explanations of what data is used, how it is collected, and why<br/>- source of data and its labels<br/>- who the labelers were and whether bias tests were conducted to assess if the labelled data was biased (e.g., bias assessment)<br/>- how data is transformed over time <br/>- risk management if the origin of data is difficult to be established",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Trace the data used by the AI system to make a certain decision(s) or recommendation(s)",
              "processes": [
                {
                  "pid": "3.4.1",
                  "process": "Log and capture clearly the data used to train a model version, as well as produce inference results using the model (batch scoring or API endpoint)",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of data used. <br/><br/>Data (raw and synthetic data) includes:   <br/>- data used to train the AI model; <br/>- data used to produce inference results using the AI model (batch scoring or API endpoint)",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Trace the AI model or rules that led to the decision(s) or recommendation(s) of the AI system",
              "processes": [
                {
                  "pid": "3.5.1",
                  "process": "Link the inference results of the model (batch scoring or API endpoint) back to the underlying model and source code",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of linking the inference results of the model (batch scoring or API endpoint) back to the underlying model and source code",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place adequate logging practices to record the decision(s) or recommendation(s) of the AI system",
              "processes": [
                {
                  "pid": "3.6.1",
                  "process": "Log all inputs and inference outputs of the model, and store them for a reasonable duration so that they can be reviewed",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of log records covering all inputs and inference outputs of the model.<br/><br/>Log records would cover: <br/>- decisions(s) of AI system; and/or<br/>- recommendation(s) of the AI system<br/>- if a human accepted or rejected AI recommendations/decisions, especially when human-in-the-loop is required",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            }
          ]
        },
        {
          "section": "Reproducibility",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Reproduce the training process for every evaluated model (except data)",
              "processes": [
                {
                  "pid": "3.7.1",
                  "process": "Version control model artefacts by associating each artefact with the version of code, dependencies, and parameters used in training",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of version control model artefacts. <br/><br/>Each artefact includes: <br/>- version of code<br/>- dependencies; and <br/>- parameters used in training",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Assess for repeatability by reviewing if the model produces the same output based on the same input  (Note: this is not relevant when it's time to the retrain model)",
              "processes": [
                {
                  "pid": "3.8.1",
                  "process": "Calculate multiple inferences. If the data follows a normal distribution, the accepted limits of this difference (or 95% of it at least) are +/-1.96 times the standard deviation of the differences between the means of the two tests",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of assessment conducted to review if the model produces the same output based on the same input",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Define the process for developing models and evaluate the process",
              "processes": [
                {
                  "pid": "3.9.1",
                  "process": "Identify a combination of technical metrics and business metrics that AI models are designed to be assessed against",
                  "metric": "Internal documentation.",
                  "processChecks": "Documentary evidence of metrics of AI models that are designed to be assessed against. <br/><br/>Metrics include: <br/>- technical metrics; and/or <br/>- business metrics",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                },
                {
                  "pid": "3.9.2",
                  "process": "Keep track of experiments (e.g., hyperparameters and model performance) used to develop challenger models, document the rationale for developing these models, and how the final deployed model was arrived at",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of the process in developing the AI model.<br/><br/>The process includes: <br/>- hyperparameters, model performance, and other relevant aspects used to develop challenger models;<br/>- the rationale for developing these models; and<br/>- how the final deployed model was derived",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy for reproducing the input data used in the training process for every model",
              "processes": [
                {
                  "pid": "3.10.1",
                  "process": "Version control the input data used for training where possible. If not possible, avoid changing the raw data at the source, and keep track of the various stages or transformation steps that are part of the data pipeline for AI model development, preferably as a directed acyclic graph (DAG)",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of having implemented a strategy for reproducing the input data used in the training process for every model.<br/><br/>This strategy includes:<br/>- data cleaning, data processing, and feature engineering<br/>- maintaining version control of the input data used for training the AI model, where possible; or<br/>- separating data manipulation process into extraction (data versioning) and processing; or<br/>- avoiding changes to the raw data at the source and keeping track of the various stages or transformation steps that are part of the data pipeline for AI model development, preferably as a directed acyclic graph (DAG).",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy for ensuring that assumptions still hold across subsequent model retraining process on new input data",
              "processes": [
                {
                  "pid": "3.11.1",
                  "process": "Record the statistical distribution of input features and output results so that divergence during retraining can be flagged. Monitor input parameters and evaluation metrics for anomalies across retraining runs",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of establishing a strategy for ensuring that assumptions still hold across subsequent model retraining process on new input data. For example:<br/>- K-L divergence and K-S test metrics can be used to compare the statistical distributions of inputs/outputs between two training runs<br/>- Moving average and standard deviations can be used to detect a significant change in model performance metrics",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Reproduce outputs of the AI system",
              "processes": [
                {
                  "pid": "3.12.1",
                  "process": "Log audit trail of when and how each model was deployed, including the code used to serve the model, testing/validation data, and what version of the model artefact was used",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of past outputs of deployed AI system, which can include: <br/>- when and how each model was deployed;<br/>- the code used to serve the model; and <br/>- the version of the model artefact used",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "If using a blackbox model or third party model, assess the vendor's claim on accuracy",
              "processes": [
                {
                  "pid": "3.13.1",
                  "process": "Curate the test set and apply the test set on the model to review performance",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of assessment conducted concerning vendor's claim on the accuracy, if using a blackbox or third party model",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy to continuously assess the quality of the output(s) of the AI system and ensure that the operating conditions of a live AI system match the thesis under which it was originally developed",
              "processes": [
                {
                  "pid": "3.14.1",
                  "process": "Continuous monitoring and periodic validation should be conducted even after models have gone live. This includes:<br/>- Model performance, e.g., monitor feature drift, inference drift, accuracy against ground truth<br/>- Application performance, e.g,, latency, throughput, error rates",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the conduct of continuous monitoring and periodic validation even after models have gone live.<br/><br/>This can include: <br/>-  Notifications to admins when a model/system exceeds some thresholds and the system is paused (if safe to do so) until the model can be improved. Any decisions that have been made/implemented while the AI was below a threshold should be flagged for reevaluation and potentially redress/remediation if harm occurred<br/>- Model performance (e.g.,monitor feature drift, inference drift, accuracy against ground truth)<br/>- Application performance (e.g., latency, throughput, error rates)",
                  "completed": "Not Applicable",
                  "elaboration": "test"
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": "Summary Justification"
    },
    {
      "id": "",
      "cid": "safety_process_checklist",
      "name": "Safety Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Safety",
      "description": "Safety is about ensuring AI systems do not cause any harm, especially physical harm. All systems will have some level of residual risk and must be developed with a preventative approach to risks that are not tolerable. Safety is achieved by reducing risks to a tolerable level. Usually, the higher the perceived risks of a system causing harm, the higher the demands on risk mitigation. The testable criteria section in this section adopt a risk-based approach to assess the appropriate level of tolerable risk, as well as identify and mitigate potential harm throughout the AI lifecycle. \t  ",
      "sections": [
        {
          "section": "Risk assessment",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Carry out an assessment of materiality on key stakeholders",
              "processes": [
                {
                  "pid": "4.1.1",
                  "process": "Complete and submit the Assessment of Materiality to the appropriate parties who are accountable for the AI system (e.g., AI governance committee, AI system owner, and reviewers) and highlight the risks of the proposed AI solution. Document the justifications for decisions on materiality and the application of relevant governance and controls to demonstrate to regulators and auditors that sufficient responsibility has been taken by humans to address potential risks. The materality assessment could be based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data",
                  "metric": "1) Internal procedure manual 2) Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of details of the assessment of materiality on key stakeholders, justifications for decisions on materiality, and the application of relevant governance/controls.<br/><br/>The Assessment of Materiality includes the following impact dimensions (where applicable):<br/>- probability of harm;<br/>- severity of harm;<br/>- Likelihood of threat;<br/>- Extent of human involvement;<br/>- Complexity of AI model;<br/>- Extensiveness of impact on stakeholders;<br/>- Degree of Transparency; and <br/>- Impact on trust",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Assess risks, risk metrics, and risk levels of the AI system in each specific use case, including the dependency of a critical AI system’s decisions on its stable and reliable behaviour",
              "processes": [
                {
                  "pid": "4.2.1",
                  "process": "Document the intended use cases, capabilities, and limitations of AI models e.g., via model cards. This documentation should be stored and retrieved together with the model artefact, as well as surfaced during a review process before the model is deployed into production",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of risk assessment done for specific use cases. <br/><br/>This risk assessment includes documenting* the: <br/>- intended use cases, capabilities, and limitations of the AI model (e.g., via model cards)<br/><br/>*Note: This documentation should be stored and retrieved together with the model artefact and surfaced during a review process before the model is deployed into production",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place a process to continuously assess, measure and monitor risks, including the identification of new risks after deployment",
              "processes": [
                {
                  "pid": "4.3.1",
                  "process": "Assign a reviewer who is familiar with the downstream use case of an AI model to review the model post-deployment. This process should include model cards/documentation to ensure alignment between intended use cases at modelling and post-deployment",
                  "metric": "Internal documentation (e.g., log, register or database)",
                  "processChecks": "Documentary evidence of process for continuous risk monitoring for AI model. <br/><br/>Process includes:<br/>- Assessing, measuring, and monitoring risks at modelling stage; and<br/>- identification of new risks after the post-deployment stage",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Assess whether the AI system might fail by considering the input features and predicted outcomes to aid communication with stakeholders",
              "processes": [
                {
                  "pid": "4.4.1",
                  "process": "Where feasible, use AI models that can produce confidence score together with prediction. Low confidence scores may occur when the data contains values that are outside the range of the training data, or for data regions where there were insufficient training examples to make a robust estimate.<br/>Implement mechanisms to detect if model input represents an outlier in terms of training data, e.g., return some \"data outlier score\" with predictions",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of assessment of whether the AI system might fail by considering the input features and predicted outcomes to aid communication to stakeholders",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Plan fault tolerance via, e.g., a duplicated system or another parallel system (AI-based or ‘conventional’)",
              "processes": [
                {
                  "pid": "4.5.1",
                  "process": "Implement deployment strategies such as blue-green and canary deployments.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of:<br/>- implementation of deployment strategies such as blue-green and canary deployments<br/>- a plan for graceful failure or failover modes",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "4.5.2",
                  "process": "Maintain backup model server in blue-green deployment mode.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of maintenance of the backup model server in blue-green deployment mode",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "4.5.3",
                  "process": "Where feasible, use AI models that can produce a confidence score together with the prediction. Design the systems that are using the results of the AI model to handle cases where the model fails or has low confidence, falling back to backup model servers or sensible default behaviour.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the use of AI models that can produce a confidence score together with the prediction, and that it can fall back to backup model servers or sensible default behaviour",
                  "completed": "Yes",
                  "elaboration": "test"
                },
                {
                  "pid": "4.5.4",
                  "process": "Close the feedback loop by retraining models with ground truth obtained once models are in production.",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of closing the feedback loop by retraining models with ground truth obtained once models are in production",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Identify residual risk that cannot be mitigated and assess the organisation's tolerance for these risks",
              "processes": [
                {
                  "pid": "4.6.1",
                  "process": "Document the assessment of the residual risk and provide reasons for the tolerance level",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of assessment of residual risk and the reasons for the organisation's tolerance for these risks",
                  "completed": "Yes",
                  "elaboration": "test"
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": "Summary Justification"
    },
    {
      "id": "",
      "cid": "security_process_checklist",
      "name": "Security Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Security",
      "description": "Security risks related to AI systems can be common across other types of software development and deployment, e.g., concerns related to the confidentiality, integrity, and availability of the system and its training and output data; and general security of the underlying software and hardware for AI systems. Security risk management considerations and approaches are applicable in the design, development, deployment, evaluation, and use of AI systems. Security also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Organisations need to develop a risk-based approach to managing AI security. This involves identifying and assessing the risks associated with the use of AI systems and implementing appropriate security controls to mitigate those risks. Organisations should also define the roles and responsibilities of different stakeholders involved in securing AI systems, including developers, operators, and users of AI system. ",
      "sections": [
        {
          "section": "Security",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Ensure Team Competency",
              "processes": [
                {
                  "pid": "5.1.1",
                  "process": "Ensure that relevant team members are knowledgeable about threats, vulnerabilities, impact, and mitigation measures relevant to securing AI systems and that their knowledge is up to date<br/><br/>Relevant team members may include any employee that is involved in the model lifecycle",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that team members have relevant security knowledge and training on threats, vulnerabilities, impact, and mitigation measures relevant to securing AI systems. This can include, where applicable: <br/>- Training records<br/>- Attendance records<br/>- Assessments<br/>- Certifications<br/>- Feedback forms",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Conduct security risk assessment at the Inception of AI system development",
              "processes": [
                {
                  "pid": "5.2.1",
                  "process": "Ensure that proper risk assessment has been carried out, in accordance with the relevant industry standards. Risk mitigation steps have been taken",
                  "metric": "Internal documentation (e.g., risk assessment)",
                  "processChecks": "Documentary evidence that risk assessment has been done in accordance with the relevant industry standards/guidelines/best practices, with risk mitigation steps and factors taken. This can include:<br/>- US NIST AI Risk Management Framework<br/>- UK NCSC guidance on secure development and deployment of software applications <br/>- OWASP Secure Software Development Lifecycle (SSDLC)<br/>- CIA triad",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place security measures during the Verification and Validation of AI system development",
              "processes": [
                {
                  "pid": "5.3.1",
                  "process": "Ensure there is integrity in data and/or models and there is a chain of custody",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that data and/or models have been obtained from a trusted source. If unable to obtain data from a trusted source, document the reason and process for using synthetic or limited data. This can include practices implemented according to:<br/>- UK NCSC supply chain security guidance<br/>- ETSI GR SAI 002 Securing AI Data Supply Chain Security<br/>- UK DSTL Machine Learning with Limited Data",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.3.2",
                  "process": "Assess the integrity of acquired datasets with a robust validation and verification process",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of assessment of the integrity of acquired datasets with a robust validation and verification process:<br/>- For internal labelled data: Have multiple labellers look at each data input and generate notification where labels differ<br/>- External procured/created data: Where possible, follow NCSC supply chain security guidance to find a trusted vendor<br/>- Randomized audits of data labels to assess error rates",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place security measures during the Design and Development of AI system development",
              "processes": [
                {
                  "pid": "5.4.1",
                  "process": "Ensure that the development environment has been secured, including trust access controls",
                  "metric": "Internal documentation (e.g.,  access control management document)",
                  "processChecks": "Documentary evidence that the development environment has been secured, including trust access controls. This can include:<br/>- Secure software development practices<br/>- Monitor Common Vulnerabilities and Exposures (CVEs) associated with the software used<br/>- Secure firmware and OS<br/>- Access controls following the principle of least privilege.<br/>- Access logging and monitoring",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.4.2",
                  "process": "Ensure that the digital assets have been secured, including data at rest and data in transit",
                  "metric": "Internal documentation (e.g., asset management document)",
                  "processChecks": "Documentary evidence that the digital assets have been secured, including data at rest and data in transit. This can include:<br/>- Implementation of recognised IT standards, such as ISO 27001",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.4.3",
                  "process": "Ensure that changes to the model or data are tracked and stored in a consistent, human- readable format",
                  "metric": "Internal documentation (e.g., asset management document)",
                  "processChecks": "Documentary evidence that changes to the model or data are tracked and stored in a consistent, human-readable format. This can include the use of relevant software that:<br/>- Tracks which users have made changes<br/>- Allows for review before changes to an asset are made<br/>- Allows ‘roll back’ to a backup in case of a security incident",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.4.4",
                  "process": "Implement measures to mitigate attacks on the dataset (e.g., poisoning attacks) <br/><br/>Where possible, conduct data sanitisation to remove suspicious or irrelevant data points. Augment the dataset with new data to diversify it and make it harder for attackers to inject poison data. Store the data set securely and ensure that sensitive data is protected and anonymised. Validate the performance of the machine learning model after training to ensure that it has not been poisoned",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of details of relevant mitigation measures taken. This can include the following measures:<br/>- Data sanitisation<br/>- Dataset augmentation<br/>- Secure storage of dataset<br/>- Validation of model performance",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place security measures during the Deployment and Monitoring of AI system development",
              "processes": [
                {
                  "pid": "5.5.1",
                  "process": "Implement measures to mitigate Inference Attacks, Extraction Attacks, or equivalent",
                  "metric": "Internal documentation (e.g., log, register or database)",
                  "processChecks": "Documentary evidence of relevant mitigation measures taken, including:<br/><br/>- Monitoring for API calls and/or input queries<br/>- Internal limits on the number of queries allowed from the same IP or with similar inputs<br/>- Implementation of secure authentication and access controls to mitigate inference attacks",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.5.2",
                  "process": "Implement an alert system for anomalous behaviour (e.g., unathorised access)",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of measures taken, including: <br/>- Following appropriate guidance when applying logging and auditing logs<br/>- Reporting to the relevant stakeholders and authority when an alert has been raised or an investigation has concluded that a cyber incident has occurred<br/>- Using human-in-the-loop to investigate what automated processes flag as unusual",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.5.3",
                  "process": "Develop a vulnerability disclosure process for AI system and organisation. This will allow users to report vulnerabilities in a responsible way",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that a vulnerability disclosure process for AI system and organisation is developed, such as using UK NCSC Vulnerability Disclosure Toolkit",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place security measures for the Continual / Online Learning Model",
              "processes": [
                {
                  "pid": "5.6.1",
                  "process": "Ensure that risks associated with continuous learning have been considered (e.g., poisoning attack, model/concept drift)<br/><br/>Determine if continual learning is still justified with the proper risk mitigations implemented",
                  "metric": "Internal documentation (e.g., risk management document)",
                  "processChecks": "Documentary evidence of<br/>- Internal approval of pre-determined model performance targets <br/>- Continual learning model having achieved pre-determined performance targets before going into production",
                  "completed": "No",
                  "elaboration": "test"
                },
                {
                  "pid": "5.6.2",
                  "process": "Ensure that approved, pre-determined performance targets are achieved before a newly updated continual learning model goes into production",
                  "metric": "Internal documentation (e.g., roadmap)",
                  "processChecks": "Documentary evidence of<br/>- Internal approval of pre-determined model performance targets <br/>- Continual learning model having achieved pre-determined performance targets before going into production",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            },
            {
              "testableCriteria": "Put in place security measures for End of Life of AI System",
              "processes": [
                {
                  "pid": "5.7.1",
                  "process": "Ensure proper and secure disposal/disclosure/destruction of data and model in accordance with data privacy standards and/or relevant rules and regulations",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of proper and secure disposal/disclosure/destruction of data and model. This can include adherence to relevant standards, guidelines, rules, and regulations",
                  "completed": "No",
                  "elaboration": "test"
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": "Summary Justification"
    },
    {
      "id": "",
      "cid": "robustness_process_checklist",
      "name": "Robustness Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Robustness",
      "description": "Robustness requires that AI systems maintain its level of performance under any circumstances, including potential changes in their operating environment or the presence of other agents (human or artificial) that may interact with the AI system in an adversarial manner.  The testable criteria in this section focus on the technical robustness of the AI system throughout its AI life cycle, to assess the proper operation of a system as intended by the system owner. These testable criteria should be carried out alongside established cybersecurity testing regimes for AI systems, to ensure overall system robustness. ",
      "sections": [
        {
          "section": "Robustness",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Put in place measures to ensure the quality of data used to develop the AI system",
              "processes": [
                {
                  "pid": "6.1.1",
                  "process": "- Implement measures to ensure data is up-to-date, complete, and representative of the environment the system will be deployed in<br/>- Log training run metadata to do comparison in production, e.g., parameters, and version model to monitor model staleness<br/>- Monitor production versus training data characteristics at production stage e.g., statistical distribution, data types, and validation constraints, to detect data and concept drift",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Evidence of measures implemented that documents:<br/>- Performance metrics (e.g., accuracy, AUROC, AUPR)<br/>- Prediction confidence score, variation ratio for the original prediction, predictive entropy<br/>- That data is of high quality, up-to-date, complete, and representative of the environment the system will be deployed in",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Review factors that may lead to a low level of accuracy of the AI system and assess if it can result in critical, adversarial, or damaging consequences",
              "processes": [
                {
                  "pid": "6.2.1",
                  "process": "Document intended use cases, risks, and limitations (e.g., in model cards)",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of intended use cases, risks, and limitations in model cards",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Consider whether the AI system's operation can invalidate the data or assumptions it was trained on e.g., feedback loops, user adaptation, and adversarial attacks",
              "processes": [
                {
                  "pid": "6.3.1",
                  "process": "Document intended use cases, risks, limitations (e.g., in model cards)",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of intended use cases, risks, and limitations in model cards (e.g., in model cards)",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place a mechanism to evaluate when the AI system has been changed to merit a new review of its technical robustness",
              "processes": [
                {
                  "pid": "6.4.1",
                  "process": "Implement a review process that highlights changes in code (e.g., training, serving), input data (e.g., raw data, features), and output data (e.g., inference results, performance metrics)",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of mechanism to evaluate when an AI system has been changed to merit a new review of its technical robustness<br/><br/>Mechanism should include a review process that highlights changes in: <br/>- code (training, serving);<br/>- input data (e.g., raw data, features); and<br/>- output data ( e.g.,inference results, performance metrics)",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy to monitor and mitigate the risk of black box attacks on live AI systems",
              "processes": [
                {
                  "pid": "6.5.1",
                  "process": "Implement methods to mitigate known adversarial attacks at training time, including decisions whether to adopt / not adopt the methods.<br/><br/>Note: It may not be possible for all models (e.g.,  if the model is deterministic or with a model with high level of interactivty with users)",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of implementing methods to mitigate adversarial attacks at training time, including decisions on whether to adopt / not adopt the methods",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "6.5.2",
                  "process": "Monitor requests made to live AI system, e.g., frequency and feature distribution of queries, in order to detect whether it is being used suspiciously",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of monitoring requests made to live AI system, e.g, frequency and feature distribution of queries, in order to detect whether it is being used suspiciously",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "6.5.3",
                  "process": "Take action on users who exhibit suspicious activity, e.g., flag for review, rate-limit or block further requests, suspend user accounts",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of action taken on users who exhibit suspicious activity. <br/><br/>Possible actions include to: <br/>- flag for review;<br/>- rate-limit or block further requests; and<br/>- suspend user accounts",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "fairness_process_checklist",
      "name": "Fairness Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Fairness",
      "description": "Fairness is about designing AI systems that avoid creating or reinforcing unfair bias in the AI system, based on the intended definition of fairness for individuals or groups, that is aligned with the desired outcomes of the AI system. The testable criteria focus on testing the ability of the AI system to align with the intended fairness outcomes, throughout the AI lifecycle. ",
      "sections": [
        {
          "section": "Fairness",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Assess within-group fairness (also known as individual fairness)",
              "processes": [
                {
                  "pid": "7.1.1",
                  "process": "Apply counterfactual fairness assessment",
                  "metric": "Internal Documentation",
                  "processChecks": "Documentary evidence of counterfactual fairness assessment",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place processes to test for potential biases during the entire lifecycle of the AI system, so that practitioners can act to mitigate biases based on feedback (e.g., biases due to possible limitations stemming from the composition of the used data sets such as a lack of diversity and non-representativeness)",
              "processes": [
                {
                  "pid": "7.2.1",
                  "process": "Monitor the changes in fairness metric values in the lifecycle of the AI system.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of implemented processes to test for potential biases during the entire lifecycle of the AI system",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy for the selection of fairness metrics that are aligned with the desired outcomes of the AI system's intended application",
              "processes": [
                {
                  "pid": "7.3.1",
                  "process": "Consider using Fairness Decision Tree (e.g., AI Verify, Aequitas)  to select the appropriate metric(s) for your application",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of strategy/process undertaken to select fairness metrics that align with the desired outcomes of the AI system's intended application. For example, Binary and Multiclass Classification<br/>- Equal Parity<br/>- Disparate Impact<br/>- False Negative Rate Parity<br/>- False Positive Rate Parity<br/>- False Omission Rate Parity<br/>- False Discovery Rate Parity<br/>- True Positive Rate Parity<br/>- True Negative Rate Parity<br/>- Negative Predictive Value Parity<br/>- Positive Predictive Value Parity<br/><br/>Regression<br/>- Mean Absolute Error Parity<br/>- Mean Square Error Parity",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Define sensitive features for the organisation that are consistent with the legislation and corporate values",
              "processes": [
                {
                  "pid": "7.4.1",
                  "process": "Identify the sensitive features and their privileged and unprivileged groups where feasible.",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of identification of sensitive features and its privileged and unprivileged groups. Examples of sensitive features could include religion, nationality, birthplace, gender, and race. Also refer to country-specific guidelines e.g., Singapore's Tripartite Guidelines on Fair Employment Practices and UK Equality Act",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "7.4.2",
                  "process": "Where feasible, consult the impacted communities on the correct definition of fairness (e.g., representatives of elderly persons or persons with disabilities),  values and considerations of those impacted (e.g., individual's preference)",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of consultations conducted with impacted communities on the correct definition of fairness",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a process for identifying and selecting sub-populations between which the AI system should produce fair outcomes",
              "processes": [
                {
                  "pid": "7.5.1",
                  "process": "Define this partitioning in terms of sensitive features that models should be prohibited from being trained on, but are used in the evaluation of fairness outcomes.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the establishment of a process for identifying and selecting sub-populations between which the AI system should produce fair outcomes",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy or a set of procedures to check that the data used in the training of the AI model, is representative of the population who make up the end-users of the AI model",
              "processes": [
                {
                  "pid": "7.6.1",
                  "process": "Perform exploratory data analysis. For the sensitive feature, test the representation of each group in the data. Resample data or collect more data if a particular group is severely underrepresented.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the establishment of a strategy or a set of procedures to check that the data used in the training of the AI model, is representative of the population who make up the end-users of the AI model",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place a mechanism that allows for the flagging of issues related to bias, discrimination, or poor performance of the AI system",
              "processes": [
                {
                  "pid": "7.7.1",
                  "process": "Monitor threshold violations of fairness metrics post-deployment and for actual harms",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of <br/>- monitoring of threshold violations of fairness metrics<br/>- obtaining feedback from those impacted by the AI system, offering redress and remediation option if feasible",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place appropriate mechanisms to ensure fairness in your AI system",
              "processes": [
                {
                  "pid": "7.8.1",
                  "process": "Monitor metrics for the latest set of data for the model currently being deployed on an ongoing basis.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of monitoring metrics for the latest set of data for the model currently being deployed on an ongoing basis",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Address the risk of biases due to possible limitations stemming from the composition of the used data sets (lack of diversity, non-representativeness), by applying appropriate adjustments on data samples of minorities",
              "processes": [
                {
                  "pid": "7.9.1",
                  "process": "Where possible, handle imbalanced training sets with minorities. Examples:<br/>- Oversample minority class<br/>- Undersample majority class<br/>- Generate synthetic samples (SMOTE)",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of addressing the risk of biases due to possible limitations stemming from the composition of the used data sets (lack of diversity, non-representativeness), by applying appropriate adjustments on data samples of minorities",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "data_governance_process_checklist",
      "name": "Data Governance Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Data Governance",
      "description": "Data Governance ensures that data is properly managed over time throughout the enterprise, including establishing authority, management, and decision-making parameters related to the data used in AI systems.\n",
      "sections": [
        {
          "section": "Data Governance",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Put in place measures to ensure data quality over time",
              "processes": [
                {
                  "pid": "8.1.1",
                  "process": "Verify the quality of data used in the AI system. This may include the following: <br/>- accuracy in terms of how well the values in the dataset match the true characteristics of the entity described by the dataset<br/>- completeness in terms of attributes and items e.g., checking for missing values, duplicate records<br/>- veracity in terms of how credible the data is, including whether the data originated from a reliable source<br/>- How recently the dataset was compiled or updated<br/>- Relevance for the intended purpose<br/>- Integrity in terms of how well extraction and transformation have been performed if multiple datasets are joined;<br/>- Usability in terms of how the data are tracked and stored in a consistent, human-readable format<br/>- Providing distribution analysis e.g., feature distributions of input data",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that proves due diligence has been done to ensure the quality of data. This can include the use of relevant processes or software that:<br/>- Conducts validation schema checks<br/>- Identifies possible errors and inconsistencies at the exploratory data analysis stage before training the dataset<br/>- Assigns roles to the entire data pipeline to trace who manipulated data and by which rule<br/>- Allows for review before a change is made<br/>- Unit tests to validate that each data operation is performed correctly prior to deployment<br/>- Allow for periodic reviewing and update of datasets <br/>- Allow for continuous assessment of the quality of the input data to the AI system, including drift parameters and thresholds, where applicable",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place measures to understand the lineage of data, including knowing where the data originally came from, how it was collected, curated, and moved within the organisation over time",
              "processes": [
                {
                  "pid": "8.2.1",
                  "process": "Maintain a data provenance record to ascertain the quality of the data based on its origin and subsequent transformation. This could include the following: <br/>- Take steps to understand the meaning of and how data was collected<br/>- Document data usage and related concerns. <br/>- Ensure any data labeling is done by a representative group of labelers <br/>- Document the procedure for assessing labels for bias<br/>- Trace potential sources of errors<br/>-Update data<br/>- Attribute data to their sources",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of a data provenance record that includes the following info, where applicable:<br/>- clear explanations of what data is used, how it is collected and why<br/>- source of data and its labels<br/>- who the labelers were and whether bias tests were conducted to assess if the labelled data was biased (e.g., bias assessment)<br/>- how data is transformed over time <br/>- risk management if the origin of data is difficult to be established",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Ensure data practices comply with relevant regulatory requirements or industry standards",
              "processes": [
                {
                  "pid": "8.3.1",
                  "process": "Ensure that assessment has been carried out in accordance with the relevant regulatory requirements and/or industry standards.  Mitigation steps have been taken.",
                  "metric": "1) Internal documentation; 2) Assessment documentation or certification(s)",
                  "processChecks": "Documentary evidence that assessment has been done in accordance with the relevant data protection laws/ standards/guidelines/best practices. For example:<br/>- applicable data protection laws and regulations such as Singapore's Personal Data Protection Act, European Data Governance Act <br/>- Singapore's Data Protection Trustmark <br/>-  Asia Pacific Economic Cooperation Cross Border Privacy Rules and Privacy Recognition for Processors<br/>- OECD Privacy Principles<br/>- Recognised data governance standards from international standard bodies (e.g., ISO, US NIST, IEEE)",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Ensure team competency in data governance",
              "processes": [
                {
                  "pid": "8.4.1",
                  "process": "Ensure that relevant team members are knowledgeable about their roles and responsibilities for data governance. Relevant team members  include any employee that is involved in managing and using the data for the AI system. For example, having a data policy team to manage the tracking of data lineage with proper controls",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence that team members have relevant knowledge and training on data governance. This can include, where applicable: <br/>- Training records<br/>- Attendance records<br/>- Assessments<br/>- Certifications<br/>- Feedback forms",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "accountability_process_checklist",
      "name": "Accountability Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Accountability",
      "description": "Accountability is about having clear internal governance mechanisms for proper management oversight of the AI system’s development and deployment.",
      "sections": [
        {
          "section": "Accountability",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Establish clear internal governance mechanisms to ensure clear roles and responsibilities for the use of AI by the organisation",
              "processes": [
                {
                  "pid": "9.1.1",
                  "process": "Adapt existing structures, communication lines, procedures, and rules (e.g., three lines of defense risk management model) or implement new ones",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of adaptation or new implementation of structures, communication lines, procedures, and rules (e.g., three lines of defense risk management model)",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "9.1.2",
                  "process": "For organisations who are using AI across departments, establish an AI governance committee that comprises representatives from data science, technology, risk, and product to facilitate cross-departmental oversight for the lifecycle governance of AI systems",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of the establishment of an AI governance committee. <br/><br/>This committee should be sufficiently representative. One way to achieve this is by having representatives from:<br/>- data science;<br/>- technology;<br/>- legal and compliance;<br/>- risk and product; and<br/>- user experience research, ethics, and psychology",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "9.1.3",
                  "process": "Enable a process to report on actions or decisions that affect the AI system's outcome, and a corresponding process for the accountable party to respond to the consequences of such an outcome",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence that outlines roles, responsibilities, and key processes for <br/><br/>- the reporting on actions or decisions that affect the AI system's outcome; <br/>- the corresponding process for the accountable party to respond to the consequences of such an outcome",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish the appropriate process or governance-by-design technology to automate or facilitate the AI system’s auditability throughout its lifecycle",
              "processes": [
                {
                  "pid": "9.2.1",
                  "process": "Process or technology should handle:<br/>- Version control of code and model<br/>- Version data or maintain immutable data<br/>- Audit trail of deployment history, log inputs/outputs, associate server predictions with the originating model",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the establishment of the appropriate process or governance-by-design technology to automate or facilitate the AI system’s auditability throughout its lifecycle.<br/><br/>The process or technology should handle:<br/>- Version control of code and model;<br/>- Version data or maintain immutable data; and<br/>- Audit trail of deployment history, log inputs/outputs, associate server predictions with the originating model",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Define the policy mechanism for enforcing access rights and permissions for the various roles of users",
              "processes": [
                {
                  "pid": "9.3.1",
                  "process": "Implement fine-grained access control that aligns with various roles for users:<br/>- Access to code and data for training AI models<br/>- Access to code and data for deploying AI models<br/>- Access to different execution environments<br/>- Permission to perform various actions (e.g., launch training job, review model, deploy model server)<br/>- Permission to define access control rules and perform other administrative functions",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of the implementation of fine-grained access control that aligns with various roles for users, which include: <br/><br/>- Access to code and data for training AI models<br/>- Access to code and data for deploying AI models<br/>- Access to different execution environments<br/>- Permission to perform various actions (e.g., launch training job, review model, deploy model server)<br/>- Permission to define access control rules and perform other administrative functions",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a strategy for maintaining independent oversight over the development and deployment of AI systems",
              "processes": [
                {
                  "pid": "9.4.1",
                  "process": "Reviewers should be distinct from those who are training and deploying models. However, it is acceptable to have the same individuals training and deploying models",
                  "metric": "Internal documentation (e.g., log, register or database)",
                  "processChecks": "Documentary evidence of strategy for maintaining independent oversight over the development and deployment of AI systems",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "If you are using third-party ‘black box’ models, assess the suitability and limits of the model for your use case",
              "processes": [
                {
                  "pid": "9.5.1",
                  "process": "Evaluate the necessity of third-party models e.g., they are trained on data otherwise not accessible to your organisation ,or you do not have the requisite capability to build AI systems in-house",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of evaluation completed regarding the necessity of third-party models",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "9.5.2",
                  "process": "Demonstrate effort to understand how the third-party models were built, including 1) what data was used to train the models, 2) how the models are assessed for effectiveness and explainability 3) under what circumstances does the AI system perform poorly",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of effort undertaken to understand how the third-party models were built, which includes:<br/><br/>- what data was used to train the models;<br/>- how the models are assessed for effectiveness and explainability; and<br/>- under what circumstances does the AI system perform poorly",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish clear responsibilities between different parties within the broader supply chain – partners, suppliers, customers, third parties",
              "processes": [
                {
                  "pid": "9.6.1",
                  "process": "Responsibilities and obligations are clearly communicated to all parties related to the AI system",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of communication with relevant parties on responsibilities and obligations relating to the AI system",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish processes to ensure that its usage of services, products or materials are in support of the responsible development and use of the AI system",
              "processes": [
                {
                  "pid": "9.7.1",
                  "process": "Select AI system suppliers which align with organisation’s approach.<br/><br/>When AI system suppliers do not perform as intended, there are processes for suppliers to take remedial actions.",
                  "metric": "External correspondence",
                  "processChecks": "Documentary evidence of processes to select suppliers which align with the organisation’s approach.<br/><br/>Documentary evidence of correspondence with suppliers to take corrective action.",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Incorporate end users’ expectations/needs in the responsible development and use of the AI system",
              "processes": [
                {
                  "pid": "9.8.1",
                  "process": "Process to engage end users on their expectations and needs (e.g. providing general usage agreements that scope its use)",
                  "metric": "External / internal correspondence",
                  "processChecks": "Documentary evidence of attempts to understand end users needs and mitigate risks related to its misuse",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "human_agency_oversight_process_checklist",
      "name": "Human Agency & Oversight Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Human Agency & Oversight",
      "description": "AI systems can be used to support or influence humans in decision-making processes. AI systems that 'act' like humans also have an effect on human perception, expectation, and functionality. Human agency and oversight ensure that the human has the ability to self-assess and intervene where necessary to ensure that the AI system is used to achieve the intended goals. The human should also have the ability to improve and override the operation of the system when the AI system results in a negative outcome.",
      "sections": [
        {
          "section": "Human Agency & Oversight",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Ensure that the various parties involved in using, reviewing, and sponsoring the AI system are adequately trained and equipped with the necessary tools and information for proper oversight to:<br/>- Obtain the needed information to conduct inquiries into past decisions made and actions taken throughout the AI lifecycle<br/>- Record information on training and deploying models as part of the workflow process",
              "processes": [
                {
                  "pid": "10.1.1",
                  "process": "Put in place guided flow for documenting (i) important info via model cards, forms, SDK library; and (ii) important processes that provide objective criteria for decision-making (e.g., fairness metrics selection)",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of guided flow for documenting:<br/><br/>- important info via model cards, forms, SDK library; and <br/>- important processes that provide objective criteria for decision-making (e.g., fairness metrics selection)",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "10.1.2",
                  "process": "Implement a data management system to gather and organise relevant information based on the needs of different user roles (e.g., reviewing models, and monitoring live systems)",
                  "metric": "Internal documentation (e.g., procedure manual, log, register, or database)",
                  "processChecks": "Documentary evidence of data management system to gather and organise relevant information based on the needs of different user roles",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Ensure specific oversight and control measures to reflect the self-learning or autonomous nature of the AI system",
              "processes": [
                {
                  "pid": "10.2.1",
                  "process": "Define the role of the human in its oversight and control of the AI system (e.g., human-in-the-loop, human-out-the-loop, human-over-the-loop)",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of the definition of the role of human in oversight and control of the AI system",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "10.2.2",
                  "process": "When the AI model is making a decision for which it is significantly unsure of the answer/prediction, consider designing the system to be able to flag these cases and triage them for a human to review.",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of consideration made in the design of the AI system on its ability to flag instances when it is making a decision for which it is significantly unsure of the answer/prediction, in order that such cases be triaged for a human to review",
                  "completed": "",
                  "elaboration": ""
                },
                {
                  "pid": "10.2.3",
                  "process": "Implement mechanisms to detect if model input represents an outlier in terms of training data (e.g., return some \"data outlier score\" with predictions)",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of implementation of mechanisms to detect if model input represents an outlier in terms of training data",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Put in place a review process before AI models are put into production, where key features and properties of the AI model are shared and visualised in a way that is accessible to decision-makers within the organisation",
              "processes": [
                {
                  "pid": "10.3.1",
                  "process": "Implement a systematic review process to present performance, explainability, and fairness metrics in a way that is understandable by data science, product, legal and risk, experience research, and ethics teams",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of the implementation of a systematic review process to present performance, explainability, and fairness metrics in a way that is understandable by relevant teams (e.g., data science, product, legal and risk, experience research, and ethics teams)",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Establish a frequency and process for testing and re-evaluating AI systems",
              "processes": [
                {
                  "pid": "10.4.1",
                  "process": "After models are put into production, put in place mechanisms to review the performance of the models on an ongoing basis, either continuously or at regular intervals.<br/>Criteria could be time-based (e.g., every 2 years) or event-based (before the launch of a new AI product, after the introduction of new data, operating context has changed due to external circumstances), or when the AI system has undergone substantial modification.",
                  "metric": "Internal documentation of physical testing",
                  "processChecks": "Documentary evidence of the establishment of a frequency and process for testing and re-evaluating AI systems",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Ensure the appropriate parties who are accountable for the AI system (e.g., AI governance committee, AI system owner, and reviewers) have considered how the AI system is used to benefit humans in decision-making processes",
              "processes": [
                {
                  "pid": "10.5.1",
                  "process": "Declaration of transparency on how and where in the decision-making process the AI system is used to complement or replace the human.",
                  "metric": "1) Internal documentation (e.g., procedure manual) 2) External / internal correspondence",
                  "processChecks": "Documentary evidence of the declaration of transparency on how and where in the decision-making process the AI system is used to complement or replace the human",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "inclusive_growth_process_checklist",
      "name": "Inclusive Growth, Societal & Environmental Well-being Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Inclusive Growth, Societal & Environmental Well-being",
      "description": "Stakeholders should proactively engage in responsible stewardship of trustworthy AI in pursuit of beneficial outcomes for people and the planet, such as augmenting human capabilities and enhancing creativity, advancing inclusion of underrepresented populations, reducing economic, social, gender, and other inequalities, and protecting natural environments, thus invigorating inclusive growth, sustainable development, and well-being.",
      "sections": [
        {
          "section": "Inclusive Growth, Societal & Environmental Well-being",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Ensure that the development of AI system is for the beneficial outcomes for individuals, society, and the environment",
              "processes": [
                {
                  "pid": "11.1.1",
                  "process": "Put in place a process to determine that the development and deployment of the AI system is for the benefit of people, society, and the environment, where applicable",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of consideration of AI system's impact on individuals, society, and environment, which may include  (where applicable):<br/>- Human capabilities to learn and make decisions<br/>- Skills, jobs, and/or job quality<br/>- Creative economies<br/>- Discriminatory and/or exclusionary norms<br/>- Environmental concerns",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    },
    {
      "id": "",
      "cid": "organisational_considerations_process_checklist",
      "name": "Organisational Considerations Process Checklist",
      "group": "AI Verify Process Checklists",
      "principle": "Organisational Considerations",
      "description": "Beyond assessment of individual AI systems, organisations need to consider issues such as the use of AI (versus non-AI options), norms and expectations as well as resources to manage the use of AI",
      "sections": [
        {
          "section": "Organisational Considerations",
          "testType": "process-checklist",
          "criteria": [
            {
              "testableCriteria": "Mechanisms are in place to inventory AI systems including ensuring that they are properly resourced in areas such as (a) data; (b) tooling; (c) system & computing; and (d) in human resources, according to organisational risk priorities",
              "processes": [
                {
                  "pid": "12.1.1",
                  "process": "Put in place guided flow for documenting (i) the inventory of AI systems and necessary resources (e.g., data, tooling, system & computing and human resources) and (ii) risk priorities",
                  "metric": "Internal documentation (e.g., procedure manual)",
                  "processChecks": "Documentary evidence of considerations of resources (e.g., data, tooling, system & computing and human resources) and risk priorities",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Intended purposes, potentially beneficial uses, contexts specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented",
              "processes": [
                {
                  "pid": "12.2.1",
                  "process": "Refer to 4.1 and 4.2. In addition, document the norms and expectations in which the AI system will be deployed",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of norms and expectations of the AI system",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Scientific integrity and Test, Evaluation, Verification, and Validation (TEVV) considerations are identified and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct validation",
              "processes": [
                {
                  "pid": "12.3.1",
                  "process": "For systems that are in experimental stage, put in place a process to document the TEVV considerations",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of TEVV considerations",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Practices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented <br/><br/>Note: “AI Actors” is defined as “those who play an active role in the AI system lifecycle, including organisations and individuals that deploy or operate AI”",
              "processes": [
                {
                  "pid": "12.4.1",
                  "process": "Refer to 7.4.2, 9.1.1, 9.1.2, 9.1.3. In addition, put in place a process to engage external AI actors for feedback",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of engagement and feedback from relevant AI actors",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available",
              "processes": [
                {
                  "pid": "12.5.1",
                  "process": "For risks difficult to assess or where metrics are not available, put in place risk tracking approaches such as developing a risk reporting matrix, communicating potential risk to affected stakeholders, monitoring risk mitigation plans and reviewing status updates regularly",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of implementation of risk tracking approaches",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Resources required to manage AI risks are taken into account – along with viable non-AI alternative systems, approaches, or methods – to reduce the magnitude or likelihood of potential impacts",
              "processes": [
                {
                  "pid": "12.6.1",
                  "process": "Conduct impact assessment on the use of AI versus non-AI alternative systems, approaches, or methods, and the resources required to manage the risk of using AI",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of impact assessment",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            },
            {
              "testableCriteria": "Organisation has in place an AI policy for the development or use of AI systems that is aligned with other organisational policies",
              "processes": [
                {
                  "pid": "12.7.1",
                  "process": "Put in place documentation on the overall AI policy of the organisation, e.g., principles that guide AI-related activities, processes for handling deviations and exceptions to policy. AI policy reviewed regularly to ensure its continued suitability, adequacy and effectiveness",
                  "metric": "Internal documentation",
                  "processChecks": "Documentary evidence of overall AI policy of the organisation, e.g., principles that guide AI-related activities, processes for handling deviations and exceptions to policy.<br/><br/>Documentary evidence of the regular review of the AI policy",
                  "completed": "",
                  "elaboration": ""
                }
              ]
            }
          ]
        }
      ],
      "summaryJustification": ""
    }
  ]
}