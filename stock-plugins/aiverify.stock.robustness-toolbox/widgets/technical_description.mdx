import React, { useEffect, useState } from 'react';
import { BarChart } from 'aiverify-shared-library/charts'

import advSample from "./images/adv_samples/0.png"
import orgSample from "./images/org_samples/0.png"


export const algo_cid = "robustness_toolbox"

export const MyChart = ({ results, modelType}) => {
  const [ data, setData ] = useState([]);

  useEffect(() => {
    setData([
        {
          "name": "Original Dataset",
          "accuracy": results.org_performance,
        },
        {
          "name": "Perturbed Dataset",
          "accuracy": results.perturbed_performance
        },
      ])
  }, [results])

  const getPercentage = (field, decimalPoints=0) => {
    return (results[field]*100).toFixed(decimalPoints);
  }

  const diffScorePercent = () => {
    const diff = Math.abs(results.org_performance - results.perturbed_performance);
    return (diff*100).toFixed(2);
  }

  // Count the other way as error rate should be higher
  const diffScoreAbsolute = () => {
    const diffScore = Math.abs(results.perturbed_performance - results.org_performance);
    const percentageDiff = ((diffScore / results.perturbed_performance)*100).toFixed(2)
    return percentageDiff
  }

  let percentage = (((results.num_of_perturbed_samples - results.num_of_failed_perturbed_samples)/results.num_of_perturbed_samples)*100).toFixed(2)
  
  // How-to-read - customise the the model type
  let introduction_postpend = ""
  if(modelType == "Regression"){
    introduction_postpend += " Each bar represents the performance of the model. The longer the bar, the higher mean absolute error of the model. A robust model will achieve similar error rate for both original dataset and perturbed dataset. If you model is not robust, the error rate of the model will increase with a perturbed dataset."
  }
  else{
    introduction_postpend += " Each bar represents the performance of the model. The longer the bar, the higher accuracy of the model. A robust model will achieve similar accuracy for both original dataset and perturbed dataset. If you model is not robust, the accuracy of the model will reduce with a perturbed dataset."
  }


  // slice the array as we cannot show everything
  let threshold = 5;
  let num_of_features = results.feature_names.length;

  return (
    <>
      <div>
        Robustness Toolbox uses Boundary Attack to perturb the test dataset. Boundary Attack is an attack that starts by adding a large amount of noise to a data point intentionally to cause a model it misclassified by the model. We use Salt-and-pepper noise to create the large amount of noise. Then, it will reduce the amount of noise added while maintaining misclassification. This algorithm does not depend on the underlying model's architecture or parameters.
        <p>
        This algorithm is developed for <b>image dataset</b> but can also be used to create noise on tabular dataset. However, it is to note that testing on tabular dataset may warrant caution when interpreting the results as this is not well-tested.
        </p>

      </div>
      <br/>

      <h3>Results</h3>

      <div style={{width:"400px"}}>
        <table style={{border:0}}>
          <tbody>
            <tr>
              <td style={{fontWeight:"bold"}}>Total Number of Samples</td>
              <td>{results.num_of_perturbed_samples}</td>
            </tr>

            <tr>
              <td style={{fontWeight:"bold"}}>Successful Perturbed Rate</td>
              <td>{percentage}%</td>
            </tr>
          </tbody>
        </table>
      </div>
      <br/>
      <BarChart
          data={data}
          chartProps={{ layout:"vertical" }}
          hideLegend={true}
          yAxisProps={{ width:100 }}
          xAxisDataKey="name"
          bars={[{ dataKey:"accuracy" }]}
        />  

      {introduction_postpend}
      <br/><br/>
      <b>What it means:</b><br/>
      The test results enable the Company to understand whether the model may be affected by dataset that might be perturbed incidentally or intentionally.<br/>

      {(() => {
        if (modelType == "Regression"){
          if (results.org_performance < results.perturbed_performance) {
            return (
                <>
                  <ul>
                    <li>The original and perturbed dataset achieved a mean absolute error rate at {results.org_performance} and {results.perturbed_performance} respectively.
                  </li>  
                    <li>The model may not be robust as there seems to have a {diffScoreAbsolute()}% increase in error.</li>
                  </ul>
                </>
              )
          } else if (results.org_performance == results.perturbed_performance) {
            return (
              <>
                <ul>
                  <li>The original and perturbed dataset achieved a mean absolute error rate at {results.org_performance} and {results.perturbed_performance} respectively.</li>  
                  <li>The error rate for both datasets are the same.</li>
                </ul>
              </>
            )
          } else {
            return (
              <>
                <ul>
                  <li>The original and perturbed dataset achieved a mean absolute error rate at {results.org_performance} and {results.perturbed_performance} respectively.</li>  
                  <li>There is an increase in mean squared error at {diffScoreAbsolute()}.</li>
                </ul>
              </>
              
            )
          }
        }
        else{
          if (results.org_performance > results.perturbed_performance) {
            return (
                <>
                  <ul>
                    <li>The original and perturbed dataset achieved an accuracy of {getPercentage("org_performance")}% and {getPercentage("perturbed_performance")}% respectively.</li>
                    <li>The performance for both datasets are the same.</li>
                  </ul>
                </>
              )
          } else if (results.org_performance == results.perturbed_performance) {
            return (
              <>
                <ul>
                  <li>The original and perturbed dataset achieved an accuracy of {getPercentage("org_performance")}% and {getPercentage("perturbed_performance")}% respectively.</li>
                  <li>The performance for both datasets are the same.</li>
                </ul>
              </>
            )
          } else {
            return (
              <>
                <ul>
                  <li> The original and perturbed dataset achieved an accuracy of {getPercentage("org_performance")}% and {getPercentage("perturbed_performance")}% respectively.</li>
                  <li>There is a {diffScorePercent()}% increase in accuracy.</li>
                </ul>
              </>
              
            )
          }
        }
        
        
      })()}

      <br/>
      <h4>Example of a perturbed sample and its predicted value</h4>
      Note:
      <ul>
        <li>The perturbed sample may not be successful in changing the prediction</li>
        <li>{threshold}/{num_of_features} features will be shown in the sample below</li>
      </ul>
      {(() => {
        // Process the table for tabular dataset
        if(results.feature_names && results.feature_names.length != 0){        
          let feature_names_content = []
          let sample_values_content = []
          let perturbed_sample_values_content = []
          
          feature_names_content.push(<th>Feature Name</th>)
          for(feature_name in results.feature_names){
            feature_names_content.push(<td>{results.feature_names[feature_name]}</td>)
          }

          results.org_samples.map((sample, index) => {
            let temp_values_content = []
            
            temp_values_content.push(<th>Original #{index}</th>)
            for(value in sample){
              temp_values_content.push(<td>{sample[value]}</td>)
            }
            
            if(temp_values_content.length > threshold){
              temp_values_content = temp_values_content.slice(0, threshold)
            }

            sample_values_content.push(temp_values_content)
          })

          results.perturbed_samples.map((sample, index) => {
            let temp_values_content = []
            
            temp_values_content.push(<th>Perturbed #{index}</th>)
            for(value in sample){
              temp_values_content.push(<td>{sample[value]}</td>)
            }
            
            if(temp_values_content.length > threshold){
              temp_values_content = temp_values_content.slice(0, threshold)
            }
            perturbed_sample_values_content.push(temp_values_content)
          })

          if(feature_names_content.length > threshold){
            feature_names_content = feature_names_content.slice(0, threshold)
          }

          console.log(sample_values_content)

          return (
            <>
              <table border = "1" style={{width:"80%", borderCollapse:'collapse'}}>
                <tbody>
                  <tr align="center">
                    {feature_names_content}
                    <td>Prediction</td>
                  </tr>
                  
                    {
                      Object.entries(sample_values_content).map((row, index) => {
                        return (
                          <>
                          <tr align="center">
                            {row[1]}
                            <td>{results.original_pred[index]}</td>
                          </tr>
                          <tr align="center">
                            {perturbed_sample_values_content[index]}
                            <td>{results.perturbed_pred[index]}</td>
                          </tr>
                          </>
                        )
                      })
                    }
                  </tbody>
              </table>
            </>
          )
        } 
        else{
          return(
            <>
              <table border="1" style={{width:"80%", borderCollapse:'collapse'}}>
                <tbody>
                  <tr align="center">
                    <td><img src={orgSample}/></td>
                    <td align="center"><img src={advSample}/></td>
                  </tr>
                  
                  <tr align="center">
                    <td>{results.original_pred[0]}</td>
                    <td align="center">Predicted as {results.perturbed_pred[0]}</td>
                  </tr>
                </tbody>
              </table>
            </>
          )
        }
      })()}

    </>
  )
}

{props.getResults(algo_cid)?(
  <>
    <div style={{ width:props.container.width, height:"200px" }}>
      <MyChart results={props.getResults(algo_cid).results} modelType={props.modelAndDatasets.model.modelType}/>
    </div>
  </>
):(
  <div>No data</div>
)}

