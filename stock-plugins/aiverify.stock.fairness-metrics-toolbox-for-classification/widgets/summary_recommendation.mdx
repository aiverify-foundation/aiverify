export const cid = "fairness_metrics_toolbox_for_classification"
export const ib_cid = "fairness_tree"
import { InterpretationChart } from './interpretationtype1.mdx';
import { InterpretationChart2 } from './interpretationtype2.mdx';


export const GetInterpretation = ({ data, container, results }) => {
  let metrics_map = {
    'False Negative Rate Parity': "False Negative Rate",
    "False Positive Rate Parity": "False Positive Rate",
    "False Discovery Rate Parity": "False Discovery Rate",
    "False Omission Rate Parity": "False Omission Rate",
    "True Positive Rate Parity / Equal Opportunity": "True Positive Rate",
    "True Negative Rate Parity": "True Negative Rate",
    "Positive Predictive Value Parity": "Positive Predictive Value Parity",
    "Negative Predictive Value Parity": "Negative Predictive Value Parity",
    "Equal Parity": "Equal Selection Parity",
    "Disparate Impact": "Disparate Impact",
  };
  let content = [];
  let metrics = data.metrics;
  
  for (var i = 0; i < metrics.length; i++) {
    // Special graph as they have more than 2 subgroups
    if (metrics[i] == "Equal Parity" || metrics[i] == "Disparate Impact"){
      content.push(<InterpretationChart2
        key={`chart-${i}`}
        metric={metrics_map[metrics[0]]}
        container={container}
        data={results}
      />)
    }
    else{
      content.push(<InterpretationChart
        key={`chart-${i}`}
        metric={metrics_map[metrics[i]]}
        container={container}
        data={results}
      />)
    }
  }

  return (
    <div>{content}</div>
  )
}

<>
  <div>
    <b>What it means:</b>
    <br/>
    The test results enable the Company to help its stakeholder understand if the model is able to predict the outcomes fairly among the demographic groups.
    <GetInterpretation
      data={props.getIBData(ib_cid)}
      container={props.container}
      results={props.getResults(cid)}
    />
  </div>

<div>
    <h4><b>Recommendations:</b> </h4>
    <p>Company can consider reviewing these fairness metrics with the relevant stakeholders so that they can better understand if the AI model has predicted outcome fairly among the sensitve features. If the parity is considered negligible and acceptable by the Company, there is no recommendation for further action. If the parity is not acceptable, consider doing the following:</p>
    <ol>
        <li>Review your dataset to identify any inherent bias in the dataset</li>
        <li>Review your model parameters and algorithms</li>
        <li>Apply post-processing mitigation algorithms (See: A Reductions Approach to Fair Classification)</li>
    </ol>
</div>
</>
