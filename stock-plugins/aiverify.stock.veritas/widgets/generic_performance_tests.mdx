import React from "react";
import {
  mockData,
  safeGetArtifactURL,
  SectionContainer,
  SectionTitle,
  SectionContent,
  formatNumber,
  ImageWithFallback,
} from "./veritas_components";

export const cid = "veritastool";

export const PerformanceMetricsTable = ({ perfMetricValues, fairnessInit }) => {
  if (!perfMetricValues || Object.keys(perfMetricValues).length === 0) {
    return <p>No performance metrics available.</p>;
  }

  const primaryMetric = fairnessInit?.perf_metric_name || "";

  return (
    <div style={{ overflowX: "auto" }}>
      <table
        style={{
          width: "100%",
          borderCollapse: "collapse",
          marginTop: "15px",
          marginBottom: "25px",
        }}
      >
        <thead>
          <tr>
            <th
              style={{
                padding: "8px 12px",
                textAlign: "left",
                borderBottom: "2px solid #ddd",
              }}
            >
              Performance Metric
            </th>
            <th
              style={{
                padding: "8px 12px",
                textAlign: "left",
                borderBottom: "2px solid #ddd",
              }}
            >
              Value
            </th>
          </tr>
        </thead>
        <tbody>
          {Object.entries(perfMetricValues).map(([metricName, valueList]) => (
            <tr
              key={metricName}
              style={{
                backgroundColor:
                  metricName === primaryMetric ? "#fff0f0" : "transparent",
                borderBottom: "1px solid #ddd",
              }}
            >
              <td
                style={{
                  padding: "8px 12px",
                  fontWeight: metricName === primaryMetric ? "bold" : "normal",
                }}
              >
                {metricName}
              </td>
              <td style={{ padding: "8px 12px" }}>
                {Array.isArray(valueList) && valueList.length >= 2
                  ? `${formatNumber(valueList[0])} Â± ${formatNumber(
                      valueList[1]
                    )}`
                  : formatNumber(valueList)}
              </td>
            </tr>
          ))}
        </tbody>
      </table>
    </div>
  );
};

export const Section = ({ title, children }) => (
  <SectionContainer>
    <SectionTitle>{title}</SectionTitle>
    <SectionContent>{children}</SectionContent>
  </SectionContainer>
);

export const PerformanceTest = ({ props }) => {
  const testResult = props.getResults ? props.getResults(cid) : null;
  const data = testResult || mockData;

  const fairness = data?.fairness || {};
  const reportPlots = fairness?.report_plots || {};

  // Helper function to safely get artifact URLs
  const getImageUrl = (pathArray) => {
    if (!pathArray || !Array.isArray(pathArray) || pathArray.length === 0)
      return null;
    return safeGetArtifactURL(props, cid, pathArray[0]);
  };

  return (
    <div
      style={{
        padding: "20px"
      }}
    >
      <h2 style={{ marginBottom: "20px" }}>
        Model Performance Analysis
      </h2>

      {/* Performance Metrics Section */}
      <Section title="Model Performance Metrics">
        <p style={{ marginBottom: "15px" }}>
          The table below lists the values and respective uncertainties of model
          performance metrics. The uncertainties in the fairness metrics are
          measured using bootstrap methods with 50 replications and 5-95%
          confidence intervals used and the plus-minus intervals representing
          two standard deviations. The primary fairness metric is highlighted.
        </p>

        <PerformanceMetricsTable
          perfMetricValues={fairness.perf_metric_values}
          fairnessInit={fairness.fairness_init}
        />
      </Section>

      {/* Confusion Matrix Section */}
      <Section title="Confusion Matrix">
        <p style={{ marginBottom: "15px" }}>
          Confusion matrix is used to describe performance of a classification
          model or classifier. It is a table comparing true outcomes with the
          predicted outcomes. The number of correct and incorrect predictions
          are summarised with count values and broken down by each class.
        </p>

        {reportPlots.confusion_matrix &&
        reportPlots.confusion_matrix.length > 0 ? (
          <div style={{ marginTop: "20px", textAlign: "center" }}>
            <div style={{ fontWeight: "bold", marginBottom: "10px" }}>
              Weighted Confusion Matrix Heatmap
            </div>
            <ImageWithFallback
              src={getImageUrl(reportPlots.confusion_matrix)}
              alt="Weighted Confusion Matrix Heatmap"
              height="350px"
            />
          </div>
        ) : (
          <p>Weighted confusion matrix is not available for this project.</p>
        )}
      </Section>

      {/* Calibration Curve Section */}
      <Section title="Calibration Curve">
        <p style={{ marginBottom: "15px" }}>
          In some situations we care about if the output represents real
          probabilities. In the reliability diagram, the x-axis represents the
          predicted probabilities, which are divided to a number of bins; the
          y-axis represents the fraction of positives in each bin. When the
          reliability diagram is close to the diagonal, then the model is well
          calibrated.
        </p>

        {reportPlots.calibration && reportPlots.calibration.length > 0 ? (
          <div style={{ marginTop: "20px", textAlign: "center" }}>
            <div style={{ fontWeight: "bold", marginBottom: "10px" }}>
              Calibration Curve
            </div>
            <ImageWithFallback
              src={getImageUrl(reportPlots.calibration)}
              alt="Calibration Curve"
              height="350px"
            />
            {fairness.calibration_curve && (
              <p style={{ marginTop: "10px" }}>
                The brier loss score is {fairness.calibration_curve.score}.
              </p>
            )}
          </div>
        ) : (
          <p>Calibration curve is not available for this project.</p>
        )}
      </Section>

      {/* Dynamic Performance Section */}
      <Section title="Dynamic Performance">
        <p style={{ marginBottom: "15px" }}>
          The following line chart illustrates the relationship between system's
          performance and system objective as threshold changes.
        </p>

        {reportPlots.performance && reportPlots.performance.length > 0 ? (
          <div style={{ marginTop: "20px", textAlign: "center" }}>
            <div style={{ fontWeight: "bold", marginBottom: "10px" }}>
              Performance
            </div>
            <ImageWithFallback
              src={getImageUrl(reportPlots.performance)}
              alt="Performance Chart"
              height="350px"
            />
          </div>
        ) : (
          <p>Performance dynamics is not available for this project.</p>
        )}
      </Section>
    </div>
  );
};

<div>
  <PerformanceTest props={props} />
</div>
