import React from "react";

export const cid = "veritastool";

export const FairnessTechnicalTestWidget = ({ props }) => {
  const testResult = props.getResults(cid)

  if (!testResult) {
    return (
      <div
        style={{
          padding: "20px",
          border: "1px solid #eee",
          borderRadius: "4px",
        }}
      >
        <h3>Fairness Technical Test</h3>
        <p>
          No test results available. Please run the Veritas fairness test to see
          results.
        </p>
      </div>
    );
  }

  // Extract fairness data from test results
  const fairnessData = testResult.fairness;
  const fairnessInit = fairnessData.fairness_init;
  const performanceMetrics = fairnessData.perf_metric_values;
  const features = fairnessData.features;
  const classDistribution = fairnessData.class_distribution;

  // Helper function to generate color for fairness conclusion
  const getFairnessColor = (conclusion) => {
    return conclusion === "fair" ? "#4CAF50" : "#F44336";
  };

  // Helper function to format decimal numbers nicely
  const formatNumber = (number) => {
    if (typeof number === "number") {
      return number.toFixed(4);
    }
    return number;
  };

  // Function to determine if the distribution is balanced
  const isBalancedDistribution = (classDistribution) => {
    const values = Object.values(classDistribution);
    const min = Math.min(...values);
    const max = Math.max(...values);
    return max / min < 2;
  };

  // Function to get class with minimum distribution
  const getMinDistributionClass = (classDistribution) => {
    let minClass = null;
    let minValue = 1;

    for (const [key, value] of Object.entries(classDistribution)) {
      if (value < minValue) {
        minValue = value;
        minClass = key;
      }
    }

    return { key: minClass, value: minValue };
  };

  // Calculate feature distribution ratio
  const getFeatureDistributionRatio = (feature) => {
    const privilegedGroup = feature.feature_distribution.privileged_group;
    const unprivilegedGroup = feature.feature_distribution.unprivileged_group;

    return (privilegedGroup / unprivilegedGroup).toFixed(2);
  };

  // Determine if feature distribution has low risk of bias
  const hasLowDistributionRisk = (feature) => {
    const ratio = getFeatureDistributionRatio(feature);
    return ratio < 2;
  };

  const minDistributionClass = getMinDistributionClass(classDistribution);
  const isBalanced = isBalancedDistribution(classDistribution);

  return (
    <div
      style={{
        fontFamily: "Arial, sans-serif",
        padding: "20px",
        overflow: "auto",
        height: props.height,
        width: props.width,
      }}
    >
      <h2>{props.properties.title || "Fairness Technical Test"}</h2>

      {/* F1.1 Systematic disadvantage explanation */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Systematic Disadvantage</h3>
        <div>
          <b>Systematic disadvantage</b> refers to a negative impact from
          unevenly distributed harms or benefits resulting from AIDA driven
          decisions across individuals or groups of customers.
        </div>
      </section>

      {/* F2.1 Confusion Matrix */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Confusion Matrix</h3>
        <div>
          For classification problems, populating the following confusion matrix
          may help. Confusion matrix is a way to measure the performance of a
          classification model. Each row of the matrix represents a predicted
          class; and each column represents an actual class. The following
          confusion matrix applies to binary classification.
        </div>

        {fairnessData.weighted_confusion_matrix && (
          <div
            style={{
              border: "1px solid #ddd",
              borderRadius: "4px",
              padding: "15px",
              marginBottom: "15px",
            }}
          >
            <div>
              <big>
                Fairness metrics for <b>Confusion Matrix</b>
              </big>
            </div>
            <table
              style={{
                width: "100%",
                borderCollapse: "collapse",
                marginTop: "10px",
              }}
            >
              <thead>
                <tr>
                  <th
                    style={{
                      border: "1px solid #ddd",
                      padding: "8px",
                      textAlign: "left",
                    }}
                  ></th>
                  <th
                    style={{
                      border: "1px solid #ddd",
                      padding: "8px",
                      textAlign: "left",
                    }}
                  >
                    Positive(Actual)
                  </th>
                  <th
                    style={{
                      border: "1px solid #ddd",
                      padding: "8px",
                      textAlign: "left",
                    }}
                  >
                    Negative(Actual)
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td
                    style={{
                      border: "1px solid #ddd",
                      padding: "8px",
                      fontWeight: "bold",
                    }}
                  >
                    Positive(Predicted)
                  </td>
                  <td style={{ border: "1px solid #ddd", padding: "8px" }}>
                    {fairnessData.weighted_confusion_matrix.tp}
                  </td>
                  <td style={{ border: "1px solid #ddd", padding: "8px" }}>
                    {fairnessData.weighted_confusion_matrix.fp}
                  </td>
                </tr>
                <tr>
                  <td
                    style={{
                      border: "1px solid #ddd",
                      padding: "8px",
                      fontWeight: "bold",
                    }}
                  >
                    Negative(Predicted)
                  </td>
                  <td style={{ border: "1px solid #ddd", padding: "8px" }}>
                    {fairnessData.weighted_confusion_matrix.fn}
                  </td>
                  <td style={{ border: "1px solid #ddd", padding: "8px" }}>
                    {fairnessData.weighted_confusion_matrix.tn}
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
        )}

        <div>
          There are four decisions:
          <ul>
            <li>
              <b>TRUE POSITIVE</b>: the real value of the sample target variable
              is positive, and the predicted value of the classification is also
              positive
            </li>
            <li>
              <b>TRUE NEGATIVE</b>: the real value of the sample target variable
              is negative, and the predicted value of the classification is also
              negative
            </li>
            <li>
              <b>FALSE POSITIVE</b>: the real value of the sample target
              variable is negative, but the predicted target value of the
              classification is positive
            </li>
            <li>
              <b>FALSE NEGATIVE</b>: the real value of the sample target
              variable is positive, but the predicted target value of the
              classification is negative
            </li>
          </ul>
        </div>
      </section>

      {/* F2.2 Harms and benefits */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Harms and Benefits</h3>
        <div>
          If the AIDA system outcome is assistive (such as assessing if an
          individual should get automated access to a financial product) then
          the most important harms to consider usually are whether at risk
          groups are incorrectly denied assistance more often than others.
          Whereas if the AIDA system outcome is punitive (such as assessing if
          an individual is fraudulent), then most important harms to consider
          usually are whether at risk groups incorrectly receive the punitive
          outcome more often than others.
        </div>

        <div style={{ marginTop: "10px" }}>
          Checking if the distribution of benefits to at risk groups is
          comparable to the target population is one way to identify important
          benefits for the groups as at risk of disadvantages.
        </div>
      </section>

      {/* F3.1 Fairness objectives */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Fairness Objectives</h3>
        <div>
          The fairness objective of AIDA system is use case specific and should
          be defined by the AIDA system Owner (usually the FSI process business
          owner). The objective must be made mathematically precise to be
          encoded into an AIDA system. The choice of fairness objectives for an
          AIDA system depends on the system's exact purpose, the consequences of
          its operation, the people affected, and the values of the people
          responsible for its design. That choice will also likely involve
          making tradeoffs between different kinds of fairness for different
          people affected by the system.
        </div>
      </section>

      {/* F3.2 Fairness metrics */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Primary Fairness Metric</h3>
        <div>
          {fairnessInit.fair_metric_name_input === "auto" ? (
            <div>
              To assess the fairness of the model, our priority is to measure{" "}
              {fairnessInit.fair_impact} {fairnessInit.fair_priority} to the{" "}
              {fairnessInit.fair_concern === "eligible"
                ? "eligible"
                : "ineligible"}{" "}
              group, therefore we choose <b>{fairnessInit.fair_metric_name}</b>{" "}
              as primary fairness metric for the assessment.
            </div>
          ) : (
            <div>
              The selected primary metric is{" "}
              <b>{fairnessInit.fair_metric_name}</b>.
            </div>
          )}
        </div>
      </section>

      {/* F4.1 Data Bias */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Data Bias Analysis</h3>
        <div>
          <ul>
            <li>
              <b>Representation bias</b> occurs when certain groups are
              underrepresented in a data set which causes the effectiveness of
              model training to be hampered.
            </li>
            <li>
              <b>Measurement bias</b> arises when there is systematic or
              non-random error in the collection of data, and can occur on input
              variables and target labels on which the AIDA system operates.
            </li>
            <li>
              <b>Pre-processing bias</b> arises during data pre-processing in
              model development, when an operation (e.g., missing value
              treatment, data cleansing, outlier treatment, encoding, scaling or
              data transformations for unstructured data, etc.) causes or
              contributes to systematic disadvantage.
            </li>
          </ul>
        </div>

        <h4>Target Label Distribution</h4>

        {classDistribution && (
          <div
            style={{
              display: "flex",
              flexDirection: "column",
              alignItems: "center",
              border: "1px solid #ddd",
              borderRadius: "4px",
              padding: "15px",
              marginBottom: "15px",
            }}
          >
            <div style={{ marginBottom: "10px" }}>Class Distribution</div>
            <div
              style={{
                width: "200px",
                height: "200px",
                position: "relative",
                borderRadius: "50%",
                background:
                  "conic-gradient(from 0deg, " +
                  `#4285F4 0% ${classDistribution.TR * 100}%, ` +
                  `#EA4335 ${classDistribution.TR * 100}% ${
                    (classDistribution.TR + classDistribution.TN) * 100
                  }%, ` +
                  `#FBBC05 ${
                    (classDistribution.TR + classDistribution.TN) * 100
                  }% ${
                    (classDistribution.TR +
                      classDistribution.TN +
                      classDistribution.CR) *
                    100
                  }%, ` +
                  `#34A853 ${
                    (classDistribution.TR +
                      classDistribution.TN +
                      classDistribution.CR) *
                    100
                  }% 100%)`,
              }}
            ></div>
            <div
              style={{
                marginTop: "10px",
                display: "flex",
                flexWrap: "wrap",
                justifyContent: "center",
              }}
            >
              {Object.entries(classDistribution).map(([key, value], index) => (
                <div
                  key={key}
                  style={{
                    display: "flex",
                    alignItems: "center",
                    margin: "5px 10px",
                  }}
                >
                  <div
                    style={{
                      width: "15px",
                      height: "15px",
                      backgroundColor:
                        index === 0
                          ? "#4285F4"
                          : index === 1
                          ? "#EA4335"
                          : index === 2
                          ? "#FBBC05"
                          : "#34A853",
                      marginRight: "5px",
                    }}
                  ></div>
                  <span>
                    {key}: {(value * 100).toFixed(2)}%
                  </span>
                </div>
              ))}
            </div>
          </div>
        )}

        {minDistributionClass.key && (
          <div>
            The proportion of <b>{minDistributionClass.key}</b> is approximately{" "}
            {formatNumber(minDistributionClass.value)},
            {isBalanced
              ? " so the imbalance in distribution of labels is small."
              : " which indicates a large imbalance in distribution of labels."}
          </div>
        )}

        <h4>Group Distribution</h4>

        <div>
          For each of the protected attribute, the group distribution is shown.
          The risk of representation bias depends on both absolute and relative
          amounts of training data. On a relative basis, less than 50 percent
          imbalance between classes is generally considered a relatively low
          level of imbalance.
        </div>

        {features &&
          Object.entries(features).map(([featureName, feature]) => (
            <div key={featureName} style={{ marginBottom: "15px" }}>
              <div
                style={{
                  display: "flex",
                  flexDirection: "column",
                  alignItems: "center",
                  border: "1px solid #ddd",
                  borderRadius: "4px",
                  padding: "15px",
                  marginBottom: "15px",
                }}
              >
                <div style={{ marginBottom: "10px" }}>
                  Feature Distribution for {featureName}
                </div>
                <div
                  style={{
                    width: "200px",
                    height: "200px",
                    position: "relative",
                    borderRadius: "50%",
                    background:
                      "conic-gradient(from 0deg, " +
                      `#4285F4 0% ${
                        feature.feature_distribution.privileged_group * 100
                      }%, ` +
                      `#EA4335 ${
                        feature.feature_distribution.privileged_group * 100
                      }% 100%)`,
                  }}
                ></div>
                <div
                  style={{
                    marginTop: "10px",
                    display: "flex",
                    flexWrap: "wrap",
                    justifyContent: "center",
                  }}
                >
                  <div
                    style={{
                      display: "flex",
                      alignItems: "center",
                      margin: "5px 10px",
                    }}
                  >
                    <div
                      style={{
                        width: "15px",
                        height: "15px",
                        backgroundColor: "#4285F4",
                        marginRight: "5px",
                      }}
                    ></div>
                    <span>
                      Privileged Group:{" "}
                      {(
                        feature.feature_distribution.privileged_group * 100
                      ).toFixed(2)}
                      %
                    </span>
                  </div>
                  <div
                    style={{
                      display: "flex",
                      alignItems: "center",
                      margin: "5px 10px",
                    }}
                  >
                    <div
                      style={{
                        width: "15px",
                        height: "15px",
                        backgroundColor: "#EA4335",
                        marginRight: "5px",
                      }}
                    ></div>
                    <span>
                      Unprivileged Group:{" "}
                      {(
                        feature.feature_distribution.unprivileged_group * 100
                      ).toFixed(2)}
                      %
                    </span>
                  </div>
                </div>
              </div>

              <div>
                The ratio of the sample size of the two groups is{" "}
                {getFeatureDistributionRatio(feature)}, which is{" "}
                {hasLowDistributionRisk(feature)
                  ? "lower than 2, so the risk of underrepresentation"
                  : "larger than 2, so there exists underrepresentation"}{" "}
                for <b>{featureName}</b>
                {hasLowDistributionRisk(feature) ? " is low." : "."}
              </div>
            </div>
          ))}
      </section>

      {/* F4.2 Proxy Bias */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Proxy Bias</h3>
        <div>
          <b>Proxy bias</b> occurs in cases where input or target variable is
          estimated using a proxy variable which is systematically different
          from the variable of interest.
        </div>
      </section>

      {/* F4.3 Historic Decision Bias */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Historic Decision Bias</h3>
        <div>
          <b>Historic decision bias</b> occurs when bias, human or otherwise,
          present in prior decisions is reflected in the training data.
        </div>
        <div style={{ marginTop: "10px" }}>
          Different sources of bias can be mitigated using different techniques.
          <ul>
            <li>
              <b>Representation bias</b>: Collect more data of underrepresented
              groups. Statistical techniques include data reweighing,
              up-sampling or down-sampling.
            </li>
            <li>
              <b>Measurement bias</b>: Obtain another, more reliable source of
              data and improve quality of instruments/sources of data collection
              in light of related best-practices.
            </li>
            <li>
              <b>Pre-processing bias</b>: Understand the reason behind anomalies
              and fixing the root causes. Another potential way to mitigate is
              augmentation or guidance from external data to make informed data
              treatments.
            </li>
            <li>
              <b>Proxy bias</b>: Change to a more relevant proxy, where the new
              proxy feature comes with an evidence based justification of
              causality, rather than a mere association with intended property
              to be measured.
            </li>
            <li>
              <b>Historic decision bias</b>: Gather more appropriate
              (potentially recent) data and adjust the selection criteria for
              the disadvantaged groups, building synthetic datasets that remove
              existing data bias, and testing AIDA model biases.
            </li>
            <li>
              Other quantitative ways include preprocessing, inprocessing and
              postprocessing bias mitigation algorithms.
            </li>
          </ul>
        </div>
      </section>

      {/* Personal Attributes */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Personal Attributes</h3>
        <div>
          Personal attributes are features about individuals that should not be
          used as the basis for decisions without reasonable justification. The
          classification of personal attributes is outlined in Veritas Document
          3A: Group 1 personal attributes will not be used in decision-making
          but can be monitored for fairness, while Group 2 personal attributes
          are justified for use in decision-making.
        </div>

        <div style={{ marginTop: "10px" }}>
          When identifying personal attributes, the following can be considered:
          laws, regulations, AIDA policies and frameworks, social cultural
          acceptance, market practices and organisational values. You can also
          refer to the Personal Attribute Identification and Classification Tree
          in Veritas Document 4.
        </div>
      </section>

      {/* F8.1 Correlation With Personal Attributes */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Correlation With Personal Attributes</h3>
        <div>
          Some of the non-personal attributes may act as proxies for personal
          attributes. So we would like to check if there are strong correlations
          for the features. The correlation matrix heatmap (if applicable) of
          top 20 most important features would be shown here.
        </div>
      </section>

      {/* F8.2 & F8.3 Feature Importance */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Feature Importance</h3>
        <div>
          Leave-One-Covariate-Out (LOCO) approach is used to measure the feature
          importance of personal attributes. Each time we train a new model by
          dropping each feature and compare the fairness metric and model
          performance metric before dropping the feature.
        </div>

        <div style={{ marginTop: "10px" }}>
          Impact on primary performance metric and primary performance metric
          are the differences between metrics before (baseline model) and after
          (LOCO model) dropping each feature (LOCO model - baseline model).
        </div>

        {features &&
          Object.entries(features).map(
            ([featureName, feature]) =>
              feature.feature_importance && (
                <div
                  key={`feature-importance-${featureName}`}
                  style={{ marginTop: "15px" }}
                >
                  <div>
                    Feature: <b>{featureName}</b>
                    <div
                      style={{
                        border: "1px solid #ddd",
                        borderRadius: "4px",
                        padding: "15px",
                        marginTop: "10px",
                      }}
                    >
                      <table
                        style={{
                          width: "100%",
                          borderCollapse: "collapse",
                        }}
                      >
                        <thead>
                          <tr>
                            <th
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                textAlign: "left",
                              }}
                            >
                              Personal attribute
                            </th>
                            <th
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                textAlign: "left",
                              }}
                            >
                              Impact on {fairnessInit.perf_metric_name}
                            </th>
                            <th
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                textAlign: "left",
                              }}
                            >
                              Impact on {fairnessInit.fair_metric_name}
                            </th>
                            <th
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                textAlign: "left",
                              }}
                            >
                              Fairness conclusion
                            </th>
                            <th
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                textAlign: "left",
                              }}
                            >
                              Suggestion
                            </th>
                          </tr>
                        </thead>
                        <tbody>
                          {Object.entries(feature.feature_importance).map(
                            ([key, value]) => (
                              <tr key={key}>
                                <td
                                  style={{
                                    border: "1px solid #ddd",
                                    padding: "8px",
                                  }}
                                >
                                  {key}
                                </td>
                                {value.map((item, index) => (
                                  <td
                                    key={index}
                                    style={{
                                      border: "1px solid #ddd",
                                      padding: "8px",
                                    }}
                                  >
                                    {item}
                                  </td>
                                ))}
                              </tr>
                            )
                          )}
                        </tbody>
                      </table>
                    </div>
                  </div>

                  <div style={{ marginTop: "10px" }}>
                    <b>{featureName}</b>: The fairness threshold is{" "}
                    {formatNumber(feature.fair_threshold)}, and the{" "}
                    <b>{fairnessInit.fair_metric_name}</b> for {featureName} in
                    the baseline model is{" "}
                    {feature.fair_metric_values &&
                    feature.fair_metric_values[fairnessInit.fair_metric_name]
                      ? formatNumber(
                          feature.fair_metric_values[
                            fairnessInit.fair_metric_name
                          ][0]
                        )
                      : "not available"}
                    {feature.fairness_conclusion === "fair"
                      ? ` which is within the threshold, so the conclusion is ${feature.fairness_conclusion}.`
                      : ` which is not within the threshold, so the conclusion is ${feature.fairness_conclusion}.`}
                  </div>
                </div>
              )
          )}
      </section>

      {/* F9.1 Primary Fairness Metric (repeat) */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Primary Fairness Metric</h3>
        <div>
          {fairnessInit.fair_metric_name_input === "auto" ? (
            <div>
              To assess the fairness of the model, our priority is to measure{" "}
              {fairnessInit.fair_impact} {fairnessInit.fair_priority} to the{" "}
              {fairnessInit.fair_concern === "eligible"
                ? "eligible"
                : "ineligible"}{" "}
              group, therefore we choose <b>{fairnessInit.fair_metric_name}</b>{" "}
              as primary fairness metric for the assessment.
            </div>
          ) : (
            <div>
              The selected primary metric is{" "}
              <b>{fairnessInit.fair_metric_name}</b>.
            </div>
          )}
        </div>
      </section>

      {/* F9.2 Fairness Metrics Values */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Group Fairness</h3>
        <div>
          The table below lists the values and respective uncertainties of
          fairness metrics. The uncertainties in the fairness metrics are
          measured using bootstrap methods with 50 replications and 5-95%
          confidence intervals used and the plus-minus intervals representing
          two standard deviations. The primary fairness metric is marked in
          bold.
        </div>

        <div style={{ marginTop: "10px" }}>
          Fairness threshold is calculated based on fairness threshold input.
          Fairness conclusion will be generated by comparing fairness threshold
          and absolute difference between the fairness metrics and neutral
          position. Note that if the metric is ratio based, neutral position is
          1; if metric is parity based, neutral position is 0); if the absolute
          difference is lower than the fairness threshold, the fairness
          conclusion would be fair.
        </div>

        {features &&
          Object.entries(features).map(([featureName, feature]) => (
            <div
              key={`fairness-metrics-${featureName}`}
              style={{ marginTop: "20px" }}
            >
              <h4>Fairness Metrics for {featureName}</h4>
              <div
                style={{
                  border: "1px solid #ddd",
                  borderRadius: "4px",
                  padding: "15px",
                }}
              >
                <table
                  style={{
                    width: "100%",
                    borderCollapse: "collapse",
                  }}
                >
                  <thead>
                    <tr>
                      <th
                        style={{
                          border: "1px solid #ddd",
                          padding: "8px",
                          textAlign: "left",
                        }}
                      >
                        Metric
                      </th>
                      <th
                        style={{
                          border: "1px solid #ddd",
                          padding: "8px",
                          textAlign: "left",
                        }}
                      >
                        Value
                      </th>
                      <th
                        style={{
                          border: "1px solid #ddd",
                          padding: "8px",
                          textAlign: "left",
                        }}
                      >
                        Uncertainty
                      </th>
                    </tr>
                  </thead>
                  <tbody>
                    {feature.fair_metric_values &&
                      Object.entries(feature.fair_metric_values).map(
                        ([metricName, values]) => (
                          <tr key={metricName}>
                            <td
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                                fontWeight:
                                  metricName === fairnessInit.fair_metric_name
                                    ? "bold"
                                    : "normal",
                                color:
                                  metricName === fairnessInit.fair_metric_name
                                    ? "#F44336"
                                    : "inherit",
                              }}
                            >
                              {metricName}
                            </td>
                            <td
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                              }}
                            >
                              {formatNumber(values[0])}
                            </td>
                            <td
                              style={{
                                border: "1px solid #ddd",
                                padding: "8px",
                              }}
                            >
                              {formatNumber(values[1])}
                            </td>
                          </tr>
                        )
                      )}
                  </tbody>
                </table>
              </div>

              <div
                style={{
                  marginTop: "10px",
                  padding: "10px",
                  backgroundColor:
                    getFairnessColor(feature.fairness_conclusion) + "20",
                  border:
                    "1px solid " +
                    getFairnessColor(feature.fairness_conclusion),
                  borderRadius: "4px",
                }}
              >
                <b>Fairness Conclusion</b>: {feature.fairness_conclusion}{" "}
                (Threshold: {formatNumber(feature.fair_threshold)})
              </div>
            </div>
          ))}

        {fairnessData.individual_fairness &&
          fairnessData.individual_fairness.consistency_score && (
            <div style={{ marginTop: "20px" }}>
              <h3>Individual Fairness</h3>
              <div>
                For individual fairness, the consistency score is{" "}
                {formatNumber(
                  fairnessData.individual_fairness.consistency_score
                )}
                .
              </div>
            </div>
          )}
      </section>

      {/* F10.1 Bias Mitigation */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Bias Mitigation</h3>
        <div>
          Pre-processing, in-processing and post-processing algorithms are
          available to mitigate the bias and improve fairness. Here the
          postprocessing mitigation method in the form of constrained balanced
          accuracy is applied. The mitigation approach selects separate
          classification thresholds for groups for which it aims to optimise
          fairness.
        </div>
      </section>

      {/* F10.2 Performance-Fairness Trade-off */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Performance-Fairness Trade-off</h3>
        <div>
          To do trade-off analysis, a grid search for thresholds was conducted.
          The objective is to bring the selected fairness metric to within the
          accepted range while maximising the balanced accuracy. The
          fairness-performance trade-offs of operating the model at various
          thresholds would be visualized below if available.
        </div>

        <ul>
          <li>
            The heatmap indicates the model's expected performance (balanced
            accuracy) when operated at each pair of risk thresholds.
          </li>
          <li>
            The white contour lines indicate the primary fairness metric with
            respect to each selected attribute. The optimal position for
            fairness metric depends on whether it is a parity-based or
            ratio-based metric. If it is parity-based, it is optimal when equal
            to zero (0); otherwise it is optimal when equal to one(1).
          </li>
          <li>
            The x-axis and y-axis show a range of possible lending risk
            thresholds for two groups, respectively.
          </li>
        </ul>

        <div style={{ marginTop: "10px" }}>
          There are three points of interests on the heatmap:
          <ul>
            <li>
              The <b>blue diamond</b> maximizes the unconstrained model
              performance.
            </li>
            <li>
              The <b>red X</b> maximizes model performance while keeping the
              same risk threshold for privileged and unprivileged group.
            </li>
            <li>
              The <b>purple star</b> maximizes the model performance while
              ensuring optimal fairness as measured via the selected fairness
              metric.
            </li>
          </ul>
        </div>
      </section>

      {/* F11 Final Fairness Outcome */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Final Fairness Outcome</h3>
        <div>
          Justify the choice of optimal trade-off position. It depends on the
          relative importance of different objectives and priorities, which the
          system is looking to address. For example, minimum acceptable
          standards for AIDA system performance and fairness standards can be
          considered.
        </div>
      </section>

      {/* F12.1 Model Monitoring */}
      <section style={{ marginBottom: "20px", pageBreakInside: "avoid" }}>
        <h3>Model Monitoring</h3>
        <div>
          This may include accuracy metrics, primary fairness metric and other
          standard fairness metrics. Disparate impact is the only fairness
          metric that can be measured without a performance period post
          deployment, as it only uses model outcomes and does not require ground
          truth.
        </div>
      </section>

      {/* Summary Section */}
      <section
        style={{
          marginTop: "30px",
          padding: "15px",
          backgroundColor: "#f8f8f8",
          borderRadius: "4px",
          pageBreakInside: "avoid",
        }}
      >
        <h3>Summary</h3>
        <div>
          Based on the fairness assessment conducted using the Veritas toolkit:
        </div>

        <ul>
          {features &&
            Object.entries(features).map(([featureName, feature]) => (
              <li key={`summary-${featureName}`}>
                Protected attribute <b>{featureName}</b>: The model is
                determined to be
                <span
                  style={{
                    fontWeight: "bold",
                    color: getFairnessColor(feature.fairness_conclusion),
                  }}
                >
                  {" "}
                  {feature.fairness_conclusion}
                </span>{" "}
                with a fairness threshold of{" "}
                {formatNumber(feature.fair_threshold)}.
              </li>
            ))}
        </ul>

        <div>
          The primary fairness metric used for this assessment was{" "}
          <b>{fairnessInit.fair_metric_name}</b>, and the primary performance
          metric was <b>{fairnessInit.perf_metric_name}</b>. The assessment was
          focused on {fairnessInit.fair_impact} {fairnessInit.fair_priority}
          to the{" "}
          {fairnessInit.fair_concern === "eligible"
            ? "eligible"
            : "ineligible"}{" "}
          group.
        </div>
      </section>
    </div>
  );
};

<div>
  <FairnessTechnicalTestWidget props={props} />
</div>